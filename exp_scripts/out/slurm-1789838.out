Namespace(batch_size=32, continue_from_epoch=88, seed=0, num_epochs=12, experiment_name='vitb16_aircraft_224_224', use_gpu=True, weight_decay_coefficient=0.0005, learning_rate=0.001, model='vitb16', pretrain='imagenet', dataloader='aircrafts', height=224, width=224)
Number of training samples:  3334
Number of validation samples:  3333
Number of testing samples:  3333
Number of classes: 100
Use Multi GPU 0
here
System learnable parameters
model.module.class_token torch.Size([1, 1, 768])
model.module.conv_proj.weight torch.Size([768, 3, 16, 16])
model.module.conv_proj.bias torch.Size([768])
model.module.encoder.pos_embedding torch.Size([1, 197, 768])
model.module.encoder.layers.encoder_layer_0.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_0.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_0.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_0.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_0.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_0.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_0.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_0.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_0.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_0.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_0.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_0.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_1.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_1.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_1.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_1.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_1.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_1.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_1.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_1.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_1.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_1.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_1.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_1.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_2.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_2.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_2.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_2.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_2.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_2.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_2.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_2.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_2.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_2.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_2.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_2.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_3.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_3.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_3.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_3.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_3.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_3.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_3.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_3.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_3.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_3.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_3.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_3.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_4.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_4.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_4.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_4.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_4.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_4.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_4.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_4.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_4.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_4.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_4.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_4.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_5.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_5.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_5.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_5.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_5.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_5.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_5.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_5.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_5.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_5.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_5.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_5.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_6.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_6.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_6.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_6.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_6.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_6.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_6.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_6.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_6.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_6.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_6.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_6.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_7.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_7.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_7.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_7.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_7.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_7.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_7.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_7.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_7.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_7.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_7.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_7.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_8.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_8.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_8.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_8.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_8.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_8.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_8.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_8.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_8.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_8.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_8.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_8.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_9.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_9.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_9.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_9.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_9.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_9.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_9.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_9.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_9.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_9.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_9.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_9.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_10.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_10.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_10.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_10.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_10.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_10.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_10.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_10.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_10.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_10.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_10.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_10.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_11.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_11.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_11.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_11.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_11.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_11.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_11.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_11.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_11.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_11.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_11.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_11.mlp.3.bias torch.Size([768])
model.module.encoder.ln.weight torch.Size([768])
model.module.encoder.ln.bias torch.Size([768])
model.module.heads.head.weight torch.Size([1000, 768])
model.module.heads.head.bias torch.Size([1000])
Total number of parameters 86567656
Total number of conv layers 1
Total number of linear layers 0
Generating test set evaluation metrics
  0%|          | 0/105 [00:00<?, ?it/s]  1%|          | 1/105 [00:11<19:27, 11.22s/it]loss: 3.3717, accuracy: 0.4688:   1%|          | 1/105 [00:11<19:27, 11.22s/it]loss: 3.3717, accuracy: 0.4688:   2%|▏         | 2/105 [00:12<09:18,  5.42s/it]loss: 1.8086, accuracy: 0.8125:   2%|▏         | 2/105 [00:12<09:18,  5.42s/it]loss: 1.8086, accuracy: 0.8125:   3%|▎         | 3/105 [00:13<06:00,  3.53s/it]loss: 2.8701, accuracy: 0.5625:   3%|▎         | 3/105 [00:13<06:00,  3.53s/it]loss: 2.8701, accuracy: 0.5625:   4%|▍         | 4/105 [00:15<04:31,  2.69s/it]loss: 2.1349, accuracy: 0.6562:   4%|▍         | 4/105 [00:15<04:31,  2.69s/it]loss: 2.1349, accuracy: 0.6562:   5%|▍         | 5/105 [00:16<03:40,  2.20s/it]loss: 2.7024, accuracy: 0.5000:   5%|▍         | 5/105 [00:16<03:40,  2.20s/it]loss: 2.7024, accuracy: 0.5000:   6%|▌         | 6/105 [00:17<03:08,  1.90s/it]loss: 2.4935, accuracy: 0.5938:   6%|▌         | 6/105 [00:17<03:08,  1.90s/it]loss: 2.4935, accuracy: 0.5938:   7%|▋         | 7/105 [00:19<02:46,  1.70s/it]loss: 1.8704, accuracy: 0.7812:   7%|▋         | 7/105 [00:19<02:46,  1.70s/it]loss: 1.8704, accuracy: 0.7812:   8%|▊         | 8/105 [00:20<02:35,  1.60s/it]loss: 2.5805, accuracy: 0.5938:   8%|▊         | 8/105 [00:20<02:35,  1.60s/it]loss: 2.5805, accuracy: 0.5938:   9%|▊         | 9/105 [00:21<02:23,  1.50s/it]loss: 2.3154, accuracy: 0.6562:   9%|▊         | 9/105 [00:21<02:23,  1.50s/it]loss: 2.3154, accuracy: 0.6562:  10%|▉         | 10/105 [00:23<02:19,  1.47s/it]loss: 2.9972, accuracy: 0.5000:  10%|▉         | 10/105 [00:23<02:19,  1.47s/it]loss: 2.9972, accuracy: 0.5000:  10%|█         | 11/105 [00:24<02:13,  1.42s/it]loss: 2.3365, accuracy: 0.7188:  10%|█         | 11/105 [00:24<02:13,  1.42s/it]loss: 2.3365, accuracy: 0.7188:  11%|█▏        | 12/105 [00:25<02:06,  1.36s/it]loss: 2.6648, accuracy: 0.5625:  11%|█▏        | 12/105 [00:25<02:06,  1.36s/it]loss: 2.6648, accuracy: 0.5625:  12%|█▏        | 13/105 [00:27<02:01,  1.32s/it]loss: 2.3840, accuracy: 0.5625:  12%|█▏        | 13/105 [00:27<02:01,  1.32s/it]loss: 2.3840, accuracy: 0.5625:  13%|█▎        | 14/105 [00:28<02:01,  1.33s/it]loss: 2.8219, accuracy: 0.4375:  13%|█▎        | 14/105 [00:28<02:01,  1.33s/it]loss: 2.8219, accuracy: 0.4375:  14%|█▍        | 15/105 [00:29<02:01,  1.35s/it]loss: 2.4380, accuracy: 0.6875:  14%|█▍        | 15/105 [00:29<02:01,  1.35s/it]loss: 2.4380, accuracy: 0.6875:  15%|█▌        | 16/105 [00:31<01:58,  1.33s/it]loss: 2.3537, accuracy: 0.7188:  15%|█▌        | 16/105 [00:31<01:58,  1.33s/it]loss: 2.3537, accuracy: 0.7188:  16%|█▌        | 17/105 [00:32<01:56,  1.32s/it]loss: 2.0780, accuracy: 0.7188:  16%|█▌        | 17/105 [00:32<01:56,  1.32s/it]loss: 2.0780, accuracy: 0.7188:  17%|█▋        | 18/105 [00:33<01:55,  1.32s/it]loss: 2.2446, accuracy: 0.7188:  17%|█▋        | 18/105 [00:33<01:55,  1.32s/it]loss: 2.2446, accuracy: 0.7188:  18%|█▊        | 19/105 [00:35<01:55,  1.34s/it]loss: 2.6354, accuracy: 0.5000:  18%|█▊        | 19/105 [00:35<01:55,  1.34s/it]loss: 2.6354, accuracy: 0.5000:  19%|█▉        | 20/105 [00:36<01:55,  1.35s/it]loss: 2.2951, accuracy: 0.6562:  19%|█▉        | 20/105 [00:36<01:55,  1.35s/it]loss: 2.2951, accuracy: 0.6562:  20%|██        | 21/105 [00:37<01:50,  1.32s/it]loss: 1.9333, accuracy: 0.7500:  20%|██        | 21/105 [00:37<01:50,  1.32s/it]loss: 1.9333, accuracy: 0.7500:  21%|██        | 22/105 [00:38<01:47,  1.30s/it]loss: 2.7925, accuracy: 0.5938:  21%|██        | 22/105 [00:38<01:47,  1.30s/it]loss: 2.7925, accuracy: 0.5938:  22%|██▏       | 23/105 [00:40<01:46,  1.30s/it]loss: 2.1383, accuracy: 0.7188:  22%|██▏       | 23/105 [00:40<01:46,  1.30s/it]loss: 2.1383, accuracy: 0.7188:  23%|██▎       | 24/105 [00:41<01:44,  1.29s/it]loss: 2.1416, accuracy: 0.7500:  23%|██▎       | 24/105 [00:41<01:44,  1.29s/it]loss: 2.1416, accuracy: 0.7500:  24%|██▍       | 25/105 [00:42<01:44,  1.30s/it]loss: 2.8237, accuracy: 0.5312:  24%|██▍       | 25/105 [00:42<01:44,  1.30s/it]loss: 2.8237, accuracy: 0.5312:  25%|██▍       | 26/105 [00:44<01:44,  1.32s/it]loss: 1.9219, accuracy: 0.8438:  25%|██▍       | 26/105 [00:44<01:44,  1.32s/it]loss: 1.9219, accuracy: 0.8438:  26%|██▌       | 27/105 [00:45<01:40,  1.28s/it]loss: 2.1327, accuracy: 0.7500:  26%|██▌       | 27/105 [00:45<01:40,  1.28s/it]loss: 2.1327, accuracy: 0.7500:  27%|██▋       | 28/105 [00:46<01:38,  1.28s/it]loss: 2.1791, accuracy: 0.7188:  27%|██▋       | 28/105 [00:46<01:38,  1.28s/it]loss: 2.1791, accuracy: 0.7188:  28%|██▊       | 29/105 [00:47<01:36,  1.27s/it]loss: 2.4740, accuracy: 0.5938:  28%|██▊       | 29/105 [00:47<01:36,  1.27s/it]loss: 2.4740, accuracy: 0.5938:  29%|██▊       | 30/105 [00:49<01:36,  1.28s/it]loss: 2.3238, accuracy: 0.7188:  29%|██▊       | 30/105 [00:49<01:36,  1.28s/it]loss: 2.3238, accuracy: 0.7188:  30%|██▉       | 31/105 [00:50<01:36,  1.30s/it]loss: 2.2877, accuracy: 0.5312:  30%|██▉       | 31/105 [00:50<01:36,  1.30s/it]loss: 2.2877, accuracy: 0.5312:  30%|███       | 32/105 [00:51<01:36,  1.33s/it]loss: 2.5570, accuracy: 0.5312:  30%|███       | 32/105 [00:51<01:36,  1.33s/it]loss: 2.5570, accuracy: 0.5312:  31%|███▏      | 33/105 [00:53<01:36,  1.34s/it]loss: 2.0947, accuracy: 0.7188:  31%|███▏      | 33/105 [00:53<01:36,  1.34s/it]loss: 2.0947, accuracy: 0.7188:  32%|███▏      | 34/105 [00:54<01:35,  1.34s/it]loss: 1.6207, accuracy: 0.8438:  32%|███▏      | 34/105 [00:54<01:35,  1.34s/it]loss: 1.6207, accuracy: 0.8438:  33%|███▎      | 35/105 [00:56<01:33,  1.33s/it]loss: 2.5508, accuracy: 0.6875:  33%|███▎      | 35/105 [00:56<01:33,  1.33s/it]loss: 2.5508, accuracy: 0.6875:  34%|███▍      | 36/105 [00:57<01:30,  1.32s/it]loss: 2.3389, accuracy: 0.6562:  34%|███▍      | 36/105 [00:57<01:30,  1.32s/it]loss: 2.3389, accuracy: 0.6562:  35%|███▌      | 37/105 [00:58<01:29,  1.32s/it]loss: 2.7218, accuracy: 0.5312:  35%|███▌      | 37/105 [00:58<01:29,  1.32s/it]loss: 2.7218, accuracy: 0.5312:  36%|███▌      | 38/105 [00:59<01:27,  1.31s/it]loss: 2.2285, accuracy: 0.7812:  36%|███▌      | 38/105 [00:59<01:27,  1.31s/it]loss: 2.2285, accuracy: 0.7812:  37%|███▋      | 39/105 [01:01<01:27,  1.33s/it]loss: 2.1836, accuracy: 0.6562:  37%|███▋      | 39/105 [01:01<01:27,  1.33s/it]loss: 2.1836, accuracy: 0.6562:  38%|███▊      | 40/105 [01:02<01:26,  1.33s/it]loss: 2.1968, accuracy: 0.6562:  38%|███▊      | 40/105 [01:02<01:26,  1.33s/it]loss: 2.1968, accuracy: 0.6562:  39%|███▉      | 41/105 [01:03<01:24,  1.32s/it]loss: 2.3369, accuracy: 0.6562:  39%|███▉      | 41/105 [01:03<01:24,  1.32s/it]loss: 2.3369, accuracy: 0.6562:  40%|████      | 42/105 [01:05<01:22,  1.31s/it]loss: 2.1558, accuracy: 0.6562:  40%|████      | 42/105 [01:05<01:22,  1.31s/it]loss: 2.1558, accuracy: 0.6562:  41%|████      | 43/105 [01:06<01:19,  1.28s/it]loss: 2.5394, accuracy: 0.5312:  41%|████      | 43/105 [01:06<01:19,  1.28s/it]loss: 2.5394, accuracy: 0.5312:  42%|████▏     | 44/105 [01:07<01:20,  1.32s/it]loss: 2.1907, accuracy: 0.7188:  42%|████▏     | 44/105 [01:07<01:20,  1.32s/it]loss: 2.1907, accuracy: 0.7188:  43%|████▎     | 45/105 [01:09<01:18,  1.31s/it]loss: 2.2102, accuracy: 0.6875:  43%|████▎     | 45/105 [01:09<01:18,  1.31s/it]loss: 2.2102, accuracy: 0.6875:  44%|████▍     | 46/105 [01:10<01:17,  1.32s/it]loss: 2.5905, accuracy: 0.6250:  44%|████▍     | 46/105 [01:10<01:17,  1.32s/it]loss: 2.5905, accuracy: 0.6250:  45%|████▍     | 47/105 [01:11<01:17,  1.33s/it]loss: 2.6961, accuracy: 0.6562:  45%|████▍     | 47/105 [01:11<01:17,  1.33s/it]loss: 2.6961, accuracy: 0.6562:  46%|████▌     | 48/105 [01:13<01:14,  1.30s/it]loss: 1.9988, accuracy: 0.6562:  46%|████▌     | 48/105 [01:13<01:14,  1.30s/it]loss: 1.9988, accuracy: 0.6562:  47%|████▋     | 49/105 [01:14<01:12,  1.30s/it]loss: 2.1694, accuracy: 0.6875:  47%|████▋     | 49/105 [01:14<01:12,  1.30s/it]loss: 2.1694, accuracy: 0.6875:  48%|████▊     | 50/105 [01:15<01:11,  1.29s/it]loss: 2.2289, accuracy: 0.7188:  48%|████▊     | 50/105 [01:15<01:11,  1.29s/it]loss: 2.2289, accuracy: 0.7188:  49%|████▊     | 51/105 [01:16<01:09,  1.30s/it]loss: 2.1746, accuracy: 0.7188:  49%|████▊     | 51/105 [01:16<01:09,  1.30s/it]loss: 2.1746, accuracy: 0.7188:  50%|████▉     | 52/105 [01:18<01:08,  1.29s/it]loss: 2.3503, accuracy: 0.6250:  50%|████▉     | 52/105 [01:18<01:08,  1.29s/it]loss: 2.3503, accuracy: 0.6250:  50%|█████     | 53/105 [01:19<01:08,  1.32s/it]loss: 1.9960, accuracy: 0.7812:  50%|█████     | 53/105 [01:19<01:08,  1.32s/it]loss: 1.9960, accuracy: 0.7812:  51%|█████▏    | 54/105 [01:20<01:07,  1.32s/it]loss: 2.3171, accuracy: 0.6562:  51%|█████▏    | 54/105 [01:20<01:07,  1.32s/it]loss: 2.3171, accuracy: 0.6562:  52%|█████▏    | 55/105 [01:22<01:05,  1.30s/it]loss: 2.6303, accuracy: 0.5625:  52%|█████▏    | 55/105 [01:22<01:05,  1.30s/it]loss: 2.6303, accuracy: 0.5625:  53%|█████▎    | 56/105 [01:23<01:05,  1.33s/it]loss: 2.9603, accuracy: 0.6250:  53%|█████▎    | 56/105 [01:23<01:05,  1.33s/it]loss: 2.9603, accuracy: 0.6250:  54%|█████▍    | 57/105 [01:24<01:03,  1.33s/it]loss: 2.4659, accuracy: 0.7188:  54%|█████▍    | 57/105 [01:24<01:03,  1.33s/it]loss: 2.4659, accuracy: 0.7188:  55%|█████▌    | 58/105 [01:26<01:01,  1.31s/it]loss: 3.1206, accuracy: 0.4375:  55%|█████▌    | 58/105 [01:26<01:01,  1.31s/it]loss: 3.1206, accuracy: 0.4375:  56%|█████▌    | 59/105 [01:27<00:59,  1.30s/it]loss: 2.7277, accuracy: 0.5938:  56%|█████▌    | 59/105 [01:27<00:59,  1.30s/it]loss: 2.7277, accuracy: 0.5938:  57%|█████▋    | 60/105 [01:28<00:58,  1.31s/it]loss: 2.1946, accuracy: 0.6875:  57%|█████▋    | 60/105 [01:28<00:58,  1.31s/it]loss: 2.1946, accuracy: 0.6875:  58%|█████▊    | 61/105 [01:30<00:56,  1.29s/it]loss: 2.4121, accuracy: 0.6250:  58%|█████▊    | 61/105 [01:30<00:56,  1.29s/it]loss: 2.4121, accuracy: 0.6250:  59%|█████▉    | 62/105 [01:31<00:55,  1.30s/it]loss: 1.7831, accuracy: 0.8125:  59%|█████▉    | 62/105 [01:31<00:55,  1.30s/it]loss: 1.7831, accuracy: 0.8125:  60%|██████    | 63/105 [01:32<00:54,  1.30s/it]loss: 2.3996, accuracy: 0.5625:  60%|██████    | 63/105 [01:32<00:54,  1.30s/it]loss: 2.3996, accuracy: 0.5625:  61%|██████    | 64/105 [01:33<00:53,  1.30s/it]loss: 2.3875, accuracy: 0.7188:  61%|██████    | 64/105 [01:33<00:53,  1.30s/it]loss: 2.3875, accuracy: 0.7188:  62%|██████▏   | 65/105 [01:35<00:51,  1.29s/it]loss: 2.6197, accuracy: 0.5938:  62%|██████▏   | 65/105 [01:35<00:51,  1.29s/it]loss: 2.6197, accuracy: 0.5938:  63%|██████▎   | 66/105 [01:36<00:50,  1.30s/it]loss: 2.4707, accuracy: 0.6562:  63%|██████▎   | 66/105 [01:36<00:50,  1.30s/it]loss: 2.4707, accuracy: 0.6562:  64%|██████▍   | 67/105 [01:37<00:49,  1.29s/it]loss: 2.4820, accuracy: 0.7188:  64%|██████▍   | 67/105 [01:37<00:49,  1.29s/it]loss: 2.4820, accuracy: 0.7188:  65%|██████▍   | 68/105 [01:39<00:48,  1.30s/it]loss: 2.2137, accuracy: 0.7812:  65%|██████▍   | 68/105 [01:39<00:48,  1.30s/it]loss: 2.2137, accuracy: 0.7812:  66%|██████▌   | 69/105 [01:40<00:46,  1.28s/it]loss: 2.1622, accuracy: 0.7812:  66%|██████▌   | 69/105 [01:40<00:46,  1.28s/it]loss: 2.1622, accuracy: 0.7812:  67%|██████▋   | 70/105 [01:41<00:44,  1.28s/it]loss: 2.0315, accuracy: 0.6875:  67%|██████▋   | 70/105 [01:41<00:44,  1.28s/it]loss: 2.0315, accuracy: 0.6875:  68%|██████▊   | 71/105 [01:42<00:43,  1.27s/it]loss: 2.4872, accuracy: 0.6875:  68%|██████▊   | 71/105 [01:42<00:43,  1.27s/it]loss: 2.4872, accuracy: 0.6875:  69%|██████▊   | 72/105 [01:44<00:41,  1.25s/it]loss: 3.0063, accuracy: 0.5000:  69%|██████▊   | 72/105 [01:44<00:41,  1.25s/it]loss: 3.0063, accuracy: 0.5000:  70%|██████▉   | 73/105 [01:45<00:40,  1.26s/it]loss: 2.2663, accuracy: 0.6562:  70%|██████▉   | 73/105 [01:45<00:40,  1.26s/it]loss: 2.2663, accuracy: 0.6562:  70%|███████   | 74/105 [01:46<00:39,  1.27s/it]loss: 2.4458, accuracy: 0.6250:  70%|███████   | 74/105 [01:46<00:39,  1.27s/it]loss: 2.4458, accuracy: 0.6250:  71%|███████▏  | 75/105 [01:47<00:37,  1.26s/it]loss: 2.4390, accuracy: 0.6562:  71%|███████▏  | 75/105 [01:47<00:37,  1.26s/it]loss: 2.4390, accuracy: 0.6562:  72%|███████▏  | 76/105 [01:49<00:36,  1.24s/it]loss: 2.5016, accuracy: 0.7188:  72%|███████▏  | 76/105 [01:49<00:36,  1.24s/it]loss: 2.5016, accuracy: 0.7188:  73%|███████▎  | 77/105 [01:50<00:34,  1.24s/it]loss: 2.7186, accuracy: 0.6562:  73%|███████▎  | 77/105 [01:50<00:34,  1.24s/it]loss: 2.7186, accuracy: 0.6562:  74%|███████▍  | 78/105 [01:51<00:33,  1.24s/it]loss: 2.2369, accuracy: 0.7500:  74%|███████▍  | 78/105 [01:51<00:33,  1.24s/it]loss: 2.2369, accuracy: 0.7500:  75%|███████▌  | 79/105 [01:52<00:32,  1.24s/it]loss: 3.0114, accuracy: 0.5000:  75%|███████▌  | 79/105 [01:52<00:32,  1.24s/it]loss: 3.0114, accuracy: 0.5000:  76%|███████▌  | 80/105 [01:54<00:32,  1.28s/it]loss: 1.8075, accuracy: 0.7500:  76%|███████▌  | 80/105 [01:54<00:32,  1.28s/it]loss: 1.8075, accuracy: 0.7500:  77%|███████▋  | 81/105 [01:55<00:30,  1.26s/it]loss: 2.2287, accuracy: 0.8125:  77%|███████▋  | 81/105 [01:55<00:30,  1.26s/it]loss: 2.2287, accuracy: 0.8125:  78%|███████▊  | 82/105 [01:56<00:28,  1.26s/it]loss: 2.6821, accuracy: 0.5625:  78%|███████▊  | 82/105 [01:56<00:28,  1.26s/it]loss: 2.6821, accuracy: 0.5625:  79%|███████▉  | 83/105 [01:57<00:27,  1.26s/it]loss: 2.6060, accuracy: 0.6250:  79%|███████▉  | 83/105 [01:57<00:27,  1.26s/it]loss: 2.6060, accuracy: 0.6250:  80%|████████  | 84/105 [01:59<00:26,  1.24s/it]loss: 2.7358, accuracy: 0.6562:  80%|████████  | 84/105 [01:59<00:26,  1.24s/it]loss: 2.7358, accuracy: 0.6562:  81%|████████  | 85/105 [02:00<00:24,  1.24s/it]loss: 1.9212, accuracy: 0.8438:  81%|████████  | 85/105 [02:00<00:24,  1.24s/it]loss: 1.9212, accuracy: 0.8438:  82%|████████▏ | 86/105 [02:01<00:23,  1.24s/it]loss: 2.8285, accuracy: 0.6250:  82%|████████▏ | 86/105 [02:01<00:23,  1.24s/it]loss: 2.8285, accuracy: 0.6250:  83%|████████▎ | 87/105 [02:02<00:21,  1.22s/it]loss: 2.0553, accuracy: 0.7812:  83%|████████▎ | 87/105 [02:02<00:21,  1.22s/it]loss: 2.0553, accuracy: 0.7812:  84%|████████▍ | 88/105 [02:04<00:20,  1.23s/it]loss: 2.7043, accuracy: 0.6250:  84%|████████▍ | 88/105 [02:04<00:20,  1.23s/it]loss: 2.7043, accuracy: 0.6250:  85%|████████▍ | 89/105 [02:05<00:19,  1.24s/it]loss: 2.7529, accuracy: 0.5938:  85%|████████▍ | 89/105 [02:05<00:19,  1.24s/it]loss: 2.7529, accuracy: 0.5938:  86%|████████▌ | 90/105 [02:06<00:18,  1.24s/it]loss: 2.7608, accuracy: 0.5000:  86%|████████▌ | 90/105 [02:06<00:18,  1.24s/it]loss: 2.7608, accuracy: 0.5000:  87%|████████▋ | 91/105 [02:07<00:17,  1.24s/it]loss: 2.2376, accuracy: 0.6562:  87%|████████▋ | 91/105 [02:07<00:17,  1.24s/it]loss: 2.2376, accuracy: 0.6562:  88%|████████▊ | 92/105 [02:09<00:16,  1.26s/it]loss: 2.3740, accuracy: 0.7188:  88%|████████▊ | 92/105 [02:09<00:16,  1.26s/it]loss: 2.3740, accuracy: 0.7188:  89%|████████▊ | 93/105 [02:10<00:15,  1.27s/it]loss: 2.2428, accuracy: 0.7188:  89%|████████▊ | 93/105 [02:10<00:15,  1.27s/it]loss: 2.2428, accuracy: 0.7188:  90%|████████▉ | 94/105 [02:11<00:13,  1.25s/it]loss: 2.1717, accuracy: 0.7188:  90%|████████▉ | 94/105 [02:11<00:13,  1.25s/it]loss: 2.1717, accuracy: 0.7188:  90%|█████████ | 95/105 [02:12<00:12,  1.23s/it]loss: 2.1555, accuracy: 0.7500:  90%|█████████ | 95/105 [02:12<00:12,  1.23s/it]loss: 2.1555, accuracy: 0.7500:  91%|█████████▏| 96/105 [02:14<00:11,  1.25s/it]loss: 2.1481, accuracy: 0.7188:  91%|█████████▏| 96/105 [02:14<00:11,  1.25s/it]loss: 2.1481, accuracy: 0.7188:  92%|█████████▏| 97/105 [02:15<00:09,  1.23s/it]loss: 2.4726, accuracy: 0.5938:  92%|█████████▏| 97/105 [02:15<00:09,  1.23s/it]loss: 2.4726, accuracy: 0.5938:  93%|█████████▎| 98/105 [02:16<00:08,  1.24s/it]loss: 3.0691, accuracy: 0.5625:  93%|█████████▎| 98/105 [02:16<00:08,  1.24s/it]loss: 3.0691, accuracy: 0.5625:  94%|█████████▍| 99/105 [02:17<00:07,  1.25s/it]loss: 2.3314, accuracy: 0.6562:  94%|█████████▍| 99/105 [02:17<00:07,  1.25s/it]loss: 2.3314, accuracy: 0.6562:  95%|█████████▌| 100/105 [02:18<00:06,  1.25s/it]loss: 2.2942, accuracy: 0.6875:  95%|█████████▌| 100/105 [02:18<00:06,  1.25s/it]loss: 2.2942, accuracy: 0.6875:  96%|█████████▌| 101/105 [02:20<00:04,  1.24s/it]loss: 2.5954, accuracy: 0.5000:  96%|█████████▌| 101/105 [02:20<00:04,  1.24s/it]loss: 2.5954, accuracy: 0.5000:  97%|█████████▋| 102/105 [02:21<00:03,  1.23s/it]loss: 2.9650, accuracy: 0.4688:  97%|█████████▋| 102/105 [02:21<00:03,  1.23s/it]loss: 2.9650, accuracy: 0.4688:  98%|█████████▊| 103/105 [02:22<00:02,  1.25s/it]loss: 2.5864, accuracy: 0.6250:  98%|█████████▊| 103/105 [02:22<00:02,  1.25s/it]loss: 2.5864, accuracy: 0.6250:  99%|█████████▉| 104/105 [02:23<00:01,  1.23s/it]loss: 2.8355, accuracy: 0.4688:  99%|█████████▉| 104/105 [02:23<00:01,  1.23s/it]loss: 2.8355, accuracy: 0.4688: 100%|██████████| 105/105 [02:25<00:00,  1.42s/it]loss: 3.0168, accuracy: 0.4000: 100%|██████████| 105/105 [02:25<00:00,  1.42s/it]loss: 3.0168, accuracy: 0.4000: 100%|██████████| 105/105 [02:25<00:00,  1.39s/it]
