Namespace(batch_size=64, continue_from_epoch=-2, seed=0, num_epochs=5, experiment_name='vitb16_base_100', use_gpu=True, weight_decay_coefficient=0.0005, learning_rate=0.001, model='vitb16', pretrain='base', dataloader='birds', height=100, width=100)
Number of training samples:  4495
Number of validation samples:  1499
Number of testing samples:  5794
Number of classes: 200
Use Multi GPU 0
here
System learnable parameters
DataParallel(
  (module): VisionTransformer(
    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (encoder): Encoder(
      (dropout): Dropout(p=0.0, inplace=False)
      (layers): Sequential(
        (encoder_layer_0): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_1): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_2): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_3): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_4): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_5): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_6): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_7): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_8): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_9): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_10): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_11): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (heads): Sequential(
      (head): Linear(in_features=768, out_features=200, bias=True)
    )
  )
)
  0%|          | 0/71 [00:00<?, ?it/s]  1%|▏         | 1/71 [00:18<21:04, 18.07s/it]loss: 1.9789, accuracy: 0.8438:   1%|▏         | 1/71 [00:18<21:04, 18.07s/it]loss: 1.9789, accuracy: 0.8438:   3%|▎         | 2/71 [00:20<10:07,  8.81s/it]loss: 1.8115, accuracy: 0.8906:   3%|▎         | 2/71 [00:20<10:07,  8.81s/it]loss: 1.8115, accuracy: 0.8906:   4%|▍         | 3/71 [00:22<06:31,  5.76s/it]loss: 1.9766, accuracy: 0.8438:   4%|▍         | 3/71 [00:22<06:31,  5.76s/it]loss: 1.9766, accuracy: 0.8438:   6%|▌         | 4/71 [00:24<04:51,  4.36s/it]loss: 1.9771, accuracy: 0.8125:   6%|▌         | 4/71 [00:24<04:51,  4.36s/it]loss: 1.9771, accuracy: 0.8125:   7%|▋         | 5/71 [00:26<03:56,  3.59s/it]loss: 1.8190, accuracy: 0.8906:   7%|▋         | 5/71 [00:26<03:56,  3.59s/it]loss: 1.8190, accuracy: 0.8906:   8%|▊         | 6/71 [00:29<03:22,  3.12s/it]loss: 2.0207, accuracy: 0.7656:   8%|▊         | 6/71 [00:29<03:22,  3.12s/it]loss: 2.0207, accuracy: 0.7656:  10%|▉         | 7/71 [00:31<03:00,  2.83s/it]loss: 1.8504, accuracy: 0.8750:  10%|▉         | 7/71 [00:31<03:00,  2.83s/it]loss: 1.8504, accuracy: 0.8750:  11%|█▏        | 8/71 [00:33<02:44,  2.61s/it]loss: 1.7335, accuracy: 0.9219:  11%|█▏        | 8/71 [00:33<02:44,  2.61s/it]loss: 1.7335, accuracy: 0.9219:  13%|█▎        | 9/71 [00:35<02:34,  2.49s/it]loss: 1.5748, accuracy: 0.9688:  13%|█▎        | 9/71 [00:35<02:34,  2.49s/it]loss: 1.5748, accuracy: 0.9688:  14%|█▍        | 10/71 [00:37<02:27,  2.41s/it]loss: 1.6637, accuracy: 0.9531:  14%|█▍        | 10/71 [00:37<02:27,  2.41s/it]loss: 1.6637, accuracy: 0.9531:  15%|█▌        | 11/71 [00:40<02:21,  2.36s/it]loss: 1.7486, accuracy: 0.9375:  15%|█▌        | 11/71 [00:40<02:21,  2.36s/it]loss: 1.7486, accuracy: 0.9375:  17%|█▋        | 12/71 [00:42<02:16,  2.31s/it]loss: 1.7962, accuracy: 0.8906:  17%|█▋        | 12/71 [00:42<02:16,  2.31s/it]loss: 1.7962, accuracy: 0.8906:  18%|█▊        | 13/71 [00:44<02:11,  2.27s/it]loss: 1.9394, accuracy: 0.8438:  18%|█▊        | 13/71 [00:44<02:11,  2.27s/it]loss: 1.9394, accuracy: 0.8438:  20%|█▉        | 14/71 [00:46<02:10,  2.30s/it]loss: 1.7771, accuracy: 0.9062:  20%|█▉        | 14/71 [00:46<02:10,  2.30s/it]loss: 1.7771, accuracy: 0.9062:  21%|██        | 15/71 [00:49<02:07,  2.27s/it]loss: 1.9455, accuracy: 0.9062:  21%|██        | 15/71 [00:49<02:07,  2.27s/it]loss: 1.9455, accuracy: 0.9062:  23%|██▎       | 16/71 [00:51<02:04,  2.26s/it]loss: 1.7773, accuracy: 0.8906:  23%|██▎       | 16/71 [00:51<02:04,  2.26s/it]loss: 1.7773, accuracy: 0.8906:  24%|██▍       | 17/71 [00:53<02:02,  2.28s/it]loss: 1.8181, accuracy: 0.8750:  24%|██▍       | 17/71 [00:53<02:02,  2.28s/it]loss: 1.8181, accuracy: 0.8750:  25%|██▌       | 18/71 [00:55<01:59,  2.26s/it]loss: 1.7806, accuracy: 0.9062:  25%|██▌       | 18/71 [00:55<01:59,  2.26s/it]loss: 1.7806, accuracy: 0.9062:  27%|██▋       | 19/71 [00:58<01:56,  2.24s/it]loss: 1.5803, accuracy: 0.9688:  27%|██▋       | 19/71 [00:58<01:56,  2.24s/it]loss: 1.5803, accuracy: 0.9688:  28%|██▊       | 20/71 [01:00<01:53,  2.22s/it]loss: 1.9251, accuracy: 0.8594:  28%|██▊       | 20/71 [01:00<01:53,  2.22s/it]loss: 1.9251, accuracy: 0.8594:  30%|██▉       | 21/71 [01:02<01:50,  2.21s/it]loss: 1.7522, accuracy: 0.8906:  30%|██▉       | 21/71 [01:02<01:50,  2.21s/it]loss: 1.7522, accuracy: 0.8906:  31%|███       | 22/71 [01:04<01:47,  2.19s/it]loss: 1.7754, accuracy: 0.9219:  31%|███       | 22/71 [01:04<01:47,  2.19s/it]loss: 1.7754, accuracy: 0.9219:  32%|███▏      | 23/71 [01:06<01:45,  2.20s/it]loss: 1.7458, accuracy: 0.9375:  32%|███▏      | 23/71 [01:06<01:45,  2.20s/it]loss: 1.7458, accuracy: 0.9375:  34%|███▍      | 24/71 [01:09<01:42,  2.19s/it]loss: 1.9427, accuracy: 0.8594:  34%|███▍      | 24/71 [01:09<01:42,  2.19s/it]loss: 1.9427, accuracy: 0.8594:  35%|███▌      | 25/71 [01:11<01:40,  2.18s/it]loss: 2.0911, accuracy: 0.7969:  35%|███▌      | 25/71 [01:11<01:40,  2.18s/it]loss: 2.0911, accuracy: 0.7969:  37%|███▋      | 26/71 [01:13<01:39,  2.21s/it]loss: 1.8525, accuracy: 0.8281:  37%|███▋      | 26/71 [01:13<01:39,  2.21s/it]loss: 1.8525, accuracy: 0.8281:  38%|███▊      | 27/71 [01:15<01:37,  2.21s/it]loss: 1.7594, accuracy: 0.9062:  38%|███▊      | 27/71 [01:15<01:37,  2.21s/it]loss: 1.7594, accuracy: 0.9062:  39%|███▉      | 28/71 [01:17<01:35,  2.22s/it]loss: 1.7376, accuracy: 0.9531:  39%|███▉      | 28/71 [01:17<01:35,  2.22s/it]loss: 1.7376, accuracy: 0.9531:  41%|████      | 29/71 [01:20<01:33,  2.22s/it]loss: 1.6442, accuracy: 0.9062:  41%|████      | 29/71 [01:20<01:33,  2.22s/it]loss: 1.6442, accuracy: 0.9062:  42%|████▏     | 30/71 [01:22<01:30,  2.22s/it]loss: 1.6964, accuracy: 0.9219:  42%|████▏     | 30/71 [01:22<01:30,  2.22s/it]loss: 1.6964, accuracy: 0.9219:  44%|████▎     | 31/71 [01:24<01:28,  2.20s/it]loss: 1.8121, accuracy: 0.8750:  44%|████▎     | 31/71 [01:24<01:28,  2.20s/it]loss: 1.8121, accuracy: 0.8750:  45%|████▌     | 32/71 [01:26<01:25,  2.20s/it]loss: 1.8629, accuracy: 0.9062:  45%|████▌     | 32/71 [01:26<01:25,  2.20s/it]loss: 1.8629, accuracy: 0.9062:  46%|████▋     | 33/71 [01:28<01:22,  2.18s/it]loss: 1.8938, accuracy: 0.8594:  46%|████▋     | 33/71 [01:28<01:22,  2.18s/it]loss: 1.8938, accuracy: 0.8594:  48%|████▊     | 34/71 [01:31<01:21,  2.19s/it]loss: 1.9758, accuracy: 0.8750:  48%|████▊     | 34/71 [01:31<01:21,  2.19s/it]loss: 1.9758, accuracy: 0.8750:  49%|████▉     | 35/71 [01:33<01:19,  2.20s/it]loss: 1.9590, accuracy: 0.7969:  49%|████▉     | 35/71 [01:33<01:19,  2.20s/it]loss: 1.9590, accuracy: 0.7969:  51%|█████     | 36/71 [01:35<01:17,  2.21s/it]loss: 1.9666, accuracy: 0.8906:  51%|█████     | 36/71 [01:35<01:17,  2.21s/it]loss: 1.9666, accuracy: 0.8906:  52%|█████▏    | 37/71 [01:37<01:15,  2.22s/it]loss: 1.8058, accuracy: 0.8750:  52%|█████▏    | 37/71 [01:37<01:15,  2.22s/it]loss: 1.8058, accuracy: 0.8750:  54%|█████▎    | 38/71 [01:39<01:12,  2.21s/it]loss: 1.9212, accuracy: 0.8594:  54%|█████▎    | 38/71 [01:39<01:12,  2.21s/it]loss: 1.9212, accuracy: 0.8594:  55%|█████▍    | 39/71 [01:42<01:12,  2.25s/it]loss: 1.9473, accuracy: 0.8281:  55%|█████▍    | 39/71 [01:42<01:12,  2.25s/it]loss: 1.9473, accuracy: 0.8281:  56%|█████▋    | 40/71 [01:44<01:09,  2.23s/it]loss: 1.6920, accuracy: 0.8906:  56%|█████▋    | 40/71 [01:44<01:09,  2.23s/it]loss: 1.6920, accuracy: 0.8906:  58%|█████▊    | 41/71 [01:46<01:06,  2.22s/it]loss: 1.7805, accuracy: 0.9062:  58%|█████▊    | 41/71 [01:46<01:06,  2.22s/it]loss: 1.7805, accuracy: 0.9062:  59%|█████▉    | 42/71 [01:48<01:03,  2.20s/it]loss: 2.0567, accuracy: 0.7969:  59%|█████▉    | 42/71 [01:48<01:03,  2.20s/it]loss: 2.0567, accuracy: 0.7969:  61%|██████    | 43/71 [01:51<01:01,  2.21s/it]loss: 2.0151, accuracy: 0.7812:  61%|██████    | 43/71 [01:51<01:01,  2.21s/it]loss: 2.0151, accuracy: 0.7812:  62%|██████▏   | 44/71 [01:53<00:59,  2.22s/it]loss: 2.2735, accuracy: 0.7031:  62%|██████▏   | 44/71 [01:53<00:59,  2.22s/it]loss: 2.2735, accuracy: 0.7031:  63%|██████▎   | 45/71 [01:55<00:58,  2.23s/it]loss: 1.8835, accuracy: 0.8750:  63%|██████▎   | 45/71 [01:55<00:58,  2.23s/it]loss: 1.8835, accuracy: 0.8750:  65%|██████▍   | 46/71 [01:57<00:55,  2.23s/it]loss: 1.8701, accuracy: 0.9062:  65%|██████▍   | 46/71 [01:57<00:55,  2.23s/it]loss: 1.8701, accuracy: 0.9062:  66%|██████▌   | 47/71 [01:59<00:52,  2.21s/it]loss: 1.7525, accuracy: 0.9219:  66%|██████▌   | 47/71 [01:59<00:52,  2.21s/it]loss: 1.7525, accuracy: 0.9219:  68%|██████▊   | 48/71 [02:02<00:50,  2.19s/it]loss: 1.8165, accuracy: 0.8750:  68%|██████▊   | 48/71 [02:02<00:50,  2.19s/it]loss: 1.8165, accuracy: 0.8750:  69%|██████▉   | 49/71 [02:04<00:48,  2.18s/it]loss: 1.8425, accuracy: 0.9062:  69%|██████▉   | 49/71 [02:04<00:48,  2.18s/it]loss: 1.8425, accuracy: 0.9062:  70%|███████   | 50/71 [02:06<00:46,  2.20s/it]loss: 1.6924, accuracy: 0.9531:  70%|███████   | 50/71 [02:06<00:46,  2.20s/it]loss: 1.6924, accuracy: 0.9531:  72%|███████▏  | 51/71 [02:08<00:44,  2.22s/it]loss: 1.6746, accuracy: 0.9375:  72%|███████▏  | 51/71 [02:08<00:44,  2.22s/it]loss: 1.6746, accuracy: 0.9375:  73%|███████▎  | 52/71 [02:11<00:42,  2.22s/it]loss: 1.9991, accuracy: 0.8438:  73%|███████▎  | 52/71 [02:11<00:42,  2.22s/it]loss: 1.9991, accuracy: 0.8438:  75%|███████▍  | 53/71 [02:13<00:39,  2.22s/it]loss: 2.0156, accuracy: 0.8906:  75%|███████▍  | 53/71 [02:13<00:39,  2.22s/it]loss: 2.0156, accuracy: 0.8906:  76%|███████▌  | 54/71 [02:15<00:37,  2.22s/it]loss: 2.1312, accuracy: 0.7500:  76%|███████▌  | 54/71 [02:15<00:37,  2.22s/it]loss: 2.1312, accuracy: 0.7500:  77%|███████▋  | 55/71 [02:17<00:35,  2.21s/it]loss: 1.9541, accuracy: 0.8438:  77%|███████▋  | 55/71 [02:17<00:35,  2.21s/it]loss: 1.9541, accuracy: 0.8438:  79%|███████▉  | 56/71 [02:19<00:33,  2.22s/it]loss: 2.1650, accuracy: 0.7031:  79%|███████▉  | 56/71 [02:19<00:33,  2.22s/it]loss: 2.1650, accuracy: 0.7031:  80%|████████  | 57/71 [02:22<00:31,  2.23s/it]loss: 1.9296, accuracy: 0.8281:  80%|████████  | 57/71 [02:22<00:31,  2.23s/it]loss: 1.9296, accuracy: 0.8281:  82%|████████▏ | 58/71 [02:24<00:28,  2.21s/it]loss: 1.7717, accuracy: 0.9062:  82%|████████▏ | 58/71 [02:24<00:28,  2.21s/it]loss: 1.7717, accuracy: 0.9062:  83%|████████▎ | 59/71 [02:26<00:26,  2.21s/it]loss: 1.7171, accuracy: 0.8906:  83%|████████▎ | 59/71 [02:26<00:26,  2.21s/it]loss: 1.7171, accuracy: 0.8906:  85%|████████▍ | 60/71 [02:28<00:24,  2.21s/it]loss: 1.6857, accuracy: 0.9531:  85%|████████▍ | 60/71 [02:28<00:24,  2.21s/it]loss: 1.6857, accuracy: 0.9531:  86%|████████▌ | 61/71 [02:30<00:22,  2.21s/it]loss: 1.8801, accuracy: 0.9062:  86%|████████▌ | 61/71 [02:30<00:22,  2.21s/it]loss: 1.8801, accuracy: 0.9062:  87%|████████▋ | 62/71 [02:33<00:19,  2.20s/it]loss: 1.9978, accuracy: 0.7969:  87%|████████▋ | 62/71 [02:33<00:19,  2.20s/it]loss: 1.9978, accuracy: 0.7969:  89%|████████▊ | 63/71 [02:35<00:17,  2.22s/it]loss: 2.0637, accuracy: 0.8438:  89%|████████▊ | 63/71 [02:35<00:17,  2.22s/it]loss: 2.0637, accuracy: 0.8438:  90%|█████████ | 64/71 [02:37<00:15,  2.23s/it]loss: 2.2019, accuracy: 0.6562:  90%|█████████ | 64/71 [02:37<00:15,  2.23s/it]loss: 2.2019, accuracy: 0.6562:  92%|█████████▏| 65/71 [02:39<00:13,  2.20s/it]loss: 2.0355, accuracy: 0.7500:  92%|█████████▏| 65/71 [02:39<00:13,  2.20s/it]loss: 2.0355, accuracy: 0.7500:  93%|█████████▎| 66/71 [02:41<00:10,  2.20s/it]loss: 1.9882, accuracy: 0.8125:  93%|█████████▎| 66/71 [02:41<00:10,  2.20s/it]loss: 1.9882, accuracy: 0.8125:  94%|█████████▍| 67/71 [02:44<00:08,  2.19s/it]loss: 1.9636, accuracy: 0.7656:  94%|█████████▍| 67/71 [02:44<00:08,  2.19s/it]loss: 1.9636, accuracy: 0.7656:  96%|█████████▌| 68/71 [02:46<00:06,  2.19s/it]loss: 1.9477, accuracy: 0.7812:  96%|█████████▌| 68/71 [02:46<00:06,  2.19s/it]loss: 1.9477, accuracy: 0.7812:  97%|█████████▋| 69/71 [02:48<00:04,  2.21s/it]loss: 1.7082, accuracy: 0.9375:  97%|█████████▋| 69/71 [02:48<00:04,  2.21s/it]loss: 1.7082, accuracy: 0.9375:  99%|█████████▊| 70/71 [02:50<00:02,  2.20s/it]loss: 1.9101, accuracy: 0.7969:  99%|█████████▊| 70/71 [02:50<00:02,  2.20s/it]loss: 1.9101, accuracy: 0.7969: 100%|██████████| 71/71 [02:52<00:00,  2.01s/it]loss: 1.2321, accuracy: 1.0000: 100%|██████████| 71/71 [02:52<00:00,  2.01s/it]loss: 1.2321, accuracy: 1.0000: 100%|██████████| 71/71 [02:52<00:00,  2.43s/it]
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:36,  1.58s/it]loss: 5.3659, accuracy: 0.1406:   4%|▍         | 1/24 [00:01<00:36,  1.58s/it]loss: 5.3659, accuracy: 0.1406:   8%|▊         | 2/24 [00:03<00:37,  1.73s/it]loss: 5.3590, accuracy: 0.0469:   8%|▊         | 2/24 [00:03<00:37,  1.73s/it]loss: 5.3590, accuracy: 0.0469:  12%|█▎        | 3/24 [00:05<00:37,  1.79s/it]loss: 5.6839, accuracy: 0.0469:  12%|█▎        | 3/24 [00:05<00:37,  1.79s/it]loss: 5.6839, accuracy: 0.0469:  17%|█▋        | 4/24 [00:07<00:35,  1.79s/it]loss: 5.3532, accuracy: 0.0938:  17%|█▋        | 4/24 [00:07<00:35,  1.79s/it]loss: 5.3532, accuracy: 0.0938:  21%|██        | 5/24 [00:08<00:34,  1.79s/it]loss: 5.3485, accuracy: 0.0938:  21%|██        | 5/24 [00:08<00:34,  1.79s/it]loss: 5.3485, accuracy: 0.0938:  25%|██▌       | 6/24 [00:10<00:32,  1.80s/it]loss: 4.8793, accuracy: 0.1719:  25%|██▌       | 6/24 [00:10<00:32,  1.80s/it]loss: 4.8793, accuracy: 0.1719:  29%|██▉       | 7/24 [00:12<00:30,  1.79s/it]loss: 5.6103, accuracy: 0.0625:  29%|██▉       | 7/24 [00:12<00:30,  1.79s/it]loss: 5.6103, accuracy: 0.0625:  33%|███▎      | 8/24 [00:14<00:29,  1.84s/it]loss: 5.3041, accuracy: 0.0625:  33%|███▎      | 8/24 [00:14<00:29,  1.84s/it]loss: 5.3041, accuracy: 0.0625:  38%|███▊      | 9/24 [00:16<00:27,  1.82s/it]