Namespace(batch_size=32, continue_from_epoch=-2, seed=0, num_epochs=12, experiment_name='vitb16_aircraft_224_224', use_gpu=True, weight_decay_coefficient=0.0005, learning_rate=0.001, model='vitb16', pretrain='imagenet', dataloader='aircrafts', height=224, width=224)
Number of training samples:  3334
Number of validation samples:  3333
Number of testing samples:  3333
Number of classes: 100
Use Multi GPU 0
here
System learnable parameters
model.module.class_token torch.Size([1, 1, 768])
model.module.conv_proj.weight torch.Size([768, 3, 16, 16])
model.module.conv_proj.bias torch.Size([768])
model.module.encoder.pos_embedding torch.Size([1, 197, 768])
model.module.encoder.layers.encoder_layer_0.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_0.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_0.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_0.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_0.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_0.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_0.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_0.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_0.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_0.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_0.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_0.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_1.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_1.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_1.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_1.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_1.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_1.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_1.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_1.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_1.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_1.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_1.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_1.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_2.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_2.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_2.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_2.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_2.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_2.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_2.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_2.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_2.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_2.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_2.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_2.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_3.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_3.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_3.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_3.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_3.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_3.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_3.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_3.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_3.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_3.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_3.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_3.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_4.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_4.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_4.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_4.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_4.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_4.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_4.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_4.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_4.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_4.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_4.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_4.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_5.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_5.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_5.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_5.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_5.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_5.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_5.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_5.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_5.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_5.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_5.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_5.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_6.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_6.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_6.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_6.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_6.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_6.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_6.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_6.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_6.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_6.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_6.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_6.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_7.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_7.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_7.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_7.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_7.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_7.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_7.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_7.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_7.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_7.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_7.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_7.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_8.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_8.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_8.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_8.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_8.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_8.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_8.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_8.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_8.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_8.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_8.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_8.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_9.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_9.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_9.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_9.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_9.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_9.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_9.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_9.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_9.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_9.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_9.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_9.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_10.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_10.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_10.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_10.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_10.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_10.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_10.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_10.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_10.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_10.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_10.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_10.mlp.3.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_11.ln_1.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_11.ln_1.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_11.self_attention.in_proj_weight torch.Size([2304, 768])
model.module.encoder.layers.encoder_layer_11.self_attention.in_proj_bias torch.Size([2304])
model.module.encoder.layers.encoder_layer_11.self_attention.out_proj.weight torch.Size([768, 768])
model.module.encoder.layers.encoder_layer_11.self_attention.out_proj.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_11.ln_2.weight torch.Size([768])
model.module.encoder.layers.encoder_layer_11.ln_2.bias torch.Size([768])
model.module.encoder.layers.encoder_layer_11.mlp.0.weight torch.Size([3072, 768])
model.module.encoder.layers.encoder_layer_11.mlp.0.bias torch.Size([3072])
model.module.encoder.layers.encoder_layer_11.mlp.3.weight torch.Size([768, 3072])
model.module.encoder.layers.encoder_layer_11.mlp.3.bias torch.Size([768])
model.module.encoder.ln.weight torch.Size([768])
model.module.encoder.ln.bias torch.Size([768])
model.module.heads.head.weight torch.Size([1000, 768])
model.module.heads.head.bias torch.Size([1000])
Total number of parameters 86567656
Total number of conv layers 1
Total number of linear layers 0
Generating test set evaluation metrics
  0%|          | 0/105 [00:00<?, ?it/s]  1%|          | 1/105 [00:19<33:57, 19.59s/it]loss: 3.3717, accuracy: 0.4688:   1%|          | 1/105 [00:19<33:57, 19.59s/it]loss: 3.3717, accuracy: 0.4688:   2%|▏         | 2/105 [00:23<17:42, 10.31s/it]loss: 1.8086, accuracy: 0.8125:   2%|▏         | 2/105 [00:23<17:42, 10.31s/it]loss: 1.8086, accuracy: 0.8125:   3%|▎         | 3/105 [00:26<12:08,  7.15s/it]loss: 2.8701, accuracy: 0.5625:   3%|▎         | 3/105 [00:26<12:08,  7.15s/it]loss: 2.8701, accuracy: 0.5625:   4%|▍         | 4/105 [00:30<09:34,  5.69s/it]loss: 2.1349, accuracy: 0.6562:   4%|▍         | 4/105 [00:30<09:34,  5.69s/it]loss: 2.1349, accuracy: 0.6562:   5%|▍         | 5/105 [00:33<08:05,  4.86s/it]loss: 2.7024, accuracy: 0.5000:   5%|▍         | 5/105 [00:33<08:05,  4.86s/it]loss: 2.7024, accuracy: 0.5000:   6%|▌         | 6/105 [00:36<07:06,  4.31s/it]loss: 2.4935, accuracy: 0.5938:   6%|▌         | 6/105 [00:36<07:06,  4.31s/it]loss: 2.4935, accuracy: 0.5938:   7%|▋         | 7/105 [00:40<06:26,  3.95s/it]loss: 1.8704, accuracy: 0.7812:   7%|▋         | 7/105 [00:40<06:26,  3.95s/it]loss: 1.8704, accuracy: 0.7812:   8%|▊         | 8/105 [00:43<06:04,  3.76s/it]loss: 2.5805, accuracy: 0.5938:   8%|▊         | 8/105 [00:43<06:04,  3.76s/it]loss: 2.5805, accuracy: 0.5938:   9%|▊         | 9/105 [00:46<05:41,  3.55s/it]loss: 2.3154, accuracy: 0.6562:   9%|▊         | 9/105 [00:46<05:41,  3.55s/it]loss: 2.3154, accuracy: 0.6562:  10%|▉         | 10/105 [00:49<05:27,  3.45s/it]loss: 2.9972, accuracy: 0.5000:  10%|▉         | 10/105 [00:49<05:27,  3.45s/it]loss: 2.9972, accuracy: 0.5000:  10%|█         | 11/105 [00:52<05:15,  3.35s/it]loss: 2.3365, accuracy: 0.7188:  10%|█         | 11/105 [00:52<05:15,  3.35s/it]loss: 2.3365, accuracy: 0.7188:  11%|█▏        | 12/105 [00:56<05:10,  3.34s/it]loss: 2.6648, accuracy: 0.5625:  11%|█▏        | 12/105 [00:56<05:10,  3.34s/it]loss: 2.6648, accuracy: 0.5625:  12%|█▏        | 13/105 [00:59<04:56,  3.22s/it]loss: 2.3840, accuracy: 0.5625:  12%|█▏        | 13/105 [00:59<04:56,  3.22s/it]loss: 2.3840, accuracy: 0.5625:  13%|█▎        | 14/105 [01:02<04:55,  3.25s/it]loss: 2.8219, accuracy: 0.4375:  13%|█▎        | 14/105 [01:02<04:55,  3.25s/it]loss: 2.8219, accuracy: 0.4375:  14%|█▍        | 15/105 [01:05<04:45,  3.17s/it]loss: 2.4380, accuracy: 0.6875:  14%|█▍        | 15/105 [01:05<04:45,  3.17s/it]loss: 2.4380, accuracy: 0.6875:  15%|█▌        | 16/105 [01:08<04:35,  3.10s/it]loss: 2.3537, accuracy: 0.7188:  15%|█▌        | 16/105 [01:08<04:35,  3.10s/it]loss: 2.3537, accuracy: 0.7188:  16%|█▌        | 17/105 [01:11<04:32,  3.09s/it]loss: 2.0780, accuracy: 0.7188:  16%|█▌        | 17/105 [01:11<04:32,  3.09s/it]loss: 2.0780, accuracy: 0.7188:  17%|█▋        | 18/105 [01:14<04:28,  3.08s/it]loss: 2.2446, accuracy: 0.7188:  17%|█▋        | 18/105 [01:14<04:28,  3.08s/it]loss: 2.2446, accuracy: 0.7188:  18%|█▊        | 19/105 [01:17<04:22,  3.05s/it]loss: 2.6354, accuracy: 0.5000:  18%|█▊        | 19/105 [01:17<04:22,  3.05s/it]loss: 2.6354, accuracy: 0.5000:  19%|█▉        | 20/105 [01:20<04:22,  3.09s/it]loss: 2.2951, accuracy: 0.6562:  19%|█▉        | 20/105 [01:20<04:22,  3.09s/it]loss: 2.2951, accuracy: 0.6562:  20%|██        | 21/105 [01:23<04:16,  3.05s/it]loss: 1.9333, accuracy: 0.7500:  20%|██        | 21/105 [01:23<04:16,  3.05s/it]loss: 1.9333, accuracy: 0.7500:  21%|██        | 22/105 [01:26<04:13,  3.06s/it]loss: 2.7925, accuracy: 0.5938:  21%|██        | 22/105 [01:26<04:13,  3.06s/it]loss: 2.7925, accuracy: 0.5938:  22%|██▏       | 23/105 [01:29<04:07,  3.01s/it]loss: 2.1383, accuracy: 0.7188:  22%|██▏       | 23/105 [01:29<04:07,  3.01s/it]loss: 2.1383, accuracy: 0.7188:  23%|██▎       | 24/105 [01:32<03:58,  2.95s/it]loss: 2.1416, accuracy: 0.7500:  23%|██▎       | 24/105 [01:32<03:58,  2.95s/it]loss: 2.1416, accuracy: 0.7500:  24%|██▍       | 25/105 [01:35<03:55,  2.94s/it]loss: 2.8237, accuracy: 0.5312:  24%|██▍       | 25/105 [01:35<03:55,  2.94s/it]loss: 2.8237, accuracy: 0.5312:  25%|██▍       | 26/105 [01:38<03:48,  2.89s/it]loss: 1.9219, accuracy: 0.8438:  25%|██▍       | 26/105 [01:38<03:48,  2.89s/it]loss: 1.9219, accuracy: 0.8438:  26%|██▌       | 27/105 [01:41<03:46,  2.91s/it]loss: 2.1327, accuracy: 0.7500:  26%|██▌       | 27/105 [01:41<03:46,  2.91s/it]loss: 2.1327, accuracy: 0.7500:  27%|██▋       | 28/105 [01:43<03:42,  2.88s/it]loss: 2.1791, accuracy: 0.7188:  27%|██▋       | 28/105 [01:43<03:42,  2.88s/it]loss: 2.1791, accuracy: 0.7188:  28%|██▊       | 29/105 [01:46<03:38,  2.87s/it]loss: 2.4740, accuracy: 0.5938:  28%|██▊       | 29/105 [01:46<03:38,  2.87s/it]loss: 2.4740, accuracy: 0.5938:  29%|██▊       | 30/105 [01:49<03:34,  2.86s/it]loss: 2.3238, accuracy: 0.7188:  29%|██▊       | 30/105 [01:49<03:34,  2.86s/it]loss: 2.3238, accuracy: 0.7188:  30%|██▉       | 31/105 [01:52<03:36,  2.92s/it]loss: 2.2877, accuracy: 0.5312:  30%|██▉       | 31/105 [01:52<03:36,  2.92s/it]loss: 2.2877, accuracy: 0.5312:  30%|███       | 32/105 [01:55<03:33,  2.92s/it]loss: 2.5570, accuracy: 0.5312:  30%|███       | 32/105 [01:55<03:33,  2.92s/it]loss: 2.5570, accuracy: 0.5312:  31%|███▏      | 33/105 [01:58<03:35,  2.99s/it]loss: 2.0947, accuracy: 0.7188:  31%|███▏      | 33/105 [01:58<03:35,  2.99s/it]loss: 2.0947, accuracy: 0.7188:  32%|███▏      | 34/105 [02:01<03:29,  2.95s/it]loss: 1.6207, accuracy: 0.8438:  32%|███▏      | 34/105 [02:01<03:29,  2.95s/it]loss: 1.6207, accuracy: 0.8438:  33%|███▎      | 35/105 [02:04<03:33,  3.06s/it]loss: 2.5508, accuracy: 0.6875:  33%|███▎      | 35/105 [02:04<03:33,  3.06s/it]loss: 2.5508, accuracy: 0.6875:  34%|███▍      | 36/105 [02:07<03:27,  3.01s/it]loss: 2.3389, accuracy: 0.6562:  34%|███▍      | 36/105 [02:07<03:27,  3.01s/it]loss: 2.3389, accuracy: 0.6562:  35%|███▌      | 37/105 [02:10<03:23,  2.99s/it]loss: 2.7218, accuracy: 0.5312:  35%|███▌      | 37/105 [02:10<03:23,  2.99s/it]loss: 2.7218, accuracy: 0.5312:  36%|███▌      | 38/105 [02:13<03:17,  2.95s/it]loss: 2.2285, accuracy: 0.7812:  36%|███▌      | 38/105 [02:13<03:17,  2.95s/it]loss: 2.2285, accuracy: 0.7812:  37%|███▋      | 39/105 [02:16<03:13,  2.93s/it]loss: 2.1836, accuracy: 0.6562:  37%|███▋      | 39/105 [02:16<03:13,  2.93s/it]loss: 2.1836, accuracy: 0.6562:  38%|███▊      | 40/105 [02:19<03:10,  2.93s/it]loss: 2.1968, accuracy: 0.6562:  38%|███▊      | 40/105 [02:19<03:10,  2.93s/it]loss: 2.1968, accuracy: 0.6562:  39%|███▉      | 41/105 [02:22<03:06,  2.91s/it]loss: 2.3369, accuracy: 0.6562:  39%|███▉      | 41/105 [02:22<03:06,  2.91s/it]loss: 2.3369, accuracy: 0.6562:  40%|████      | 42/105 [02:25<03:09,  3.01s/it]loss: 2.1558, accuracy: 0.6562:  40%|████      | 42/105 [02:25<03:09,  3.01s/it]loss: 2.1558, accuracy: 0.6562:  41%|████      | 43/105 [02:28<03:04,  2.97s/it]loss: 2.5394, accuracy: 0.5312:  41%|████      | 43/105 [02:28<03:04,  2.97s/it]loss: 2.5394, accuracy: 0.5312:  42%|████▏     | 44/105 [02:31<03:01,  2.97s/it]loss: 2.1907, accuracy: 0.7188:  42%|████▏     | 44/105 [02:31<03:01,  2.97s/it]loss: 2.1907, accuracy: 0.7188:  43%|████▎     | 45/105 [02:34<02:58,  2.97s/it]loss: 2.2102, accuracy: 0.6875:  43%|████▎     | 45/105 [02:34<02:58,  2.97s/it]loss: 2.2102, accuracy: 0.6875:  44%|████▍     | 46/105 [02:37<02:55,  2.97s/it]loss: 2.5905, accuracy: 0.6250:  44%|████▍     | 46/105 [02:37<02:55,  2.97s/it]loss: 2.5905, accuracy: 0.6250:  45%|████▍     | 47/105 [02:40<02:52,  2.97s/it]loss: 2.6961, accuracy: 0.6562:  45%|████▍     | 47/105 [02:40<02:52,  2.97s/it]loss: 2.6961, accuracy: 0.6562:  46%|████▌     | 48/105 [02:42<02:45,  2.91s/it]loss: 1.9988, accuracy: 0.6562:  46%|████▌     | 48/105 [02:42<02:45,  2.91s/it]loss: 1.9988, accuracy: 0.6562:  47%|████▋     | 49/105 [02:45<02:44,  2.93s/it]loss: 2.1694, accuracy: 0.6875:  47%|████▋     | 49/105 [02:45<02:44,  2.93s/it]loss: 2.1694, accuracy: 0.6875:  48%|████▊     | 50/105 [02:48<02:38,  2.89s/it]loss: 2.2289, accuracy: 0.7188:  48%|████▊     | 50/105 [02:48<02:38,  2.89s/it]loss: 2.2289, accuracy: 0.7188:  49%|████▊     | 51/105 [02:51<02:35,  2.89s/it]loss: 2.1746, accuracy: 0.7188:  49%|████▊     | 51/105 [02:51<02:35,  2.89s/it]loss: 2.1746, accuracy: 0.7188:  50%|████▉     | 52/105 [02:54<02:35,  2.93s/it]loss: 2.3503, accuracy: 0.6250:  50%|████▉     | 52/105 [02:54<02:35,  2.93s/it]loss: 2.3503, accuracy: 0.6250:  50%|█████     | 53/105 [02:57<02:33,  2.95s/it]loss: 1.9960, accuracy: 0.7812:  50%|█████     | 53/105 [02:57<02:33,  2.95s/it]loss: 1.9960, accuracy: 0.7812:  51%|█████▏    | 54/105 [03:00<02:29,  2.93s/it]loss: 2.3171, accuracy: 0.6562:  51%|█████▏    | 54/105 [03:00<02:29,  2.93s/it]loss: 2.3171, accuracy: 0.6562:  52%|█████▏    | 55/105 [03:03<02:26,  2.93s/it]loss: 2.6303, accuracy: 0.5625:  52%|█████▏    | 55/105 [03:03<02:26,  2.93s/it]loss: 2.6303, accuracy: 0.5625:  53%|█████▎    | 56/105 [03:06<02:23,  2.93s/it]loss: 2.9603, accuracy: 0.6250:  53%|█████▎    | 56/105 [03:06<02:23,  2.93s/it]loss: 2.9603, accuracy: 0.6250:  54%|█████▍    | 57/105 [03:09<02:17,  2.87s/it]loss: 2.4659, accuracy: 0.7188:  54%|█████▍    | 57/105 [03:09<02:17,  2.87s/it]loss: 2.4659, accuracy: 0.7188:  55%|█████▌    | 58/105 [03:11<02:14,  2.85s/it]loss: 3.1206, accuracy: 0.4375:  55%|█████▌    | 58/105 [03:11<02:14,  2.85s/it]loss: 3.1206, accuracy: 0.4375:  56%|█████▌    | 59/105 [03:15<02:13,  2.91s/it]loss: 2.7277, accuracy: 0.5938:  56%|█████▌    | 59/105 [03:15<02:13,  2.91s/it]loss: 2.7277, accuracy: 0.5938:  57%|█████▋    | 60/105 [03:17<02:10,  2.90s/it]loss: 2.1946, accuracy: 0.6875:  57%|█████▋    | 60/105 [03:17<02:10,  2.90s/it]loss: 2.1946, accuracy: 0.6875:  58%|█████▊    | 61/105 [03:20<02:09,  2.94s/it]loss: 2.4121, accuracy: 0.6250:  58%|█████▊    | 61/105 [03:20<02:09,  2.94s/it]loss: 2.4121, accuracy: 0.6250:  59%|█████▉    | 62/105 [03:23<02:04,  2.90s/it]loss: 1.7831, accuracy: 0.8125:  59%|█████▉    | 62/105 [03:23<02:04,  2.90s/it]loss: 1.7831, accuracy: 0.8125:  60%|██████    | 63/105 [03:26<02:00,  2.87s/it]loss: 2.3996, accuracy: 0.5625:  60%|██████    | 63/105 [03:26<02:00,  2.87s/it]loss: 2.3996, accuracy: 0.5625:  61%|██████    | 64/105 [03:29<01:58,  2.89s/it]loss: 2.3875, accuracy: 0.7188:  61%|██████    | 64/105 [03:29<01:58,  2.89s/it]loss: 2.3875, accuracy: 0.7188:  62%|██████▏   | 65/105 [03:32<01:52,  2.82s/it]loss: 2.6197, accuracy: 0.5938:  62%|██████▏   | 65/105 [03:32<01:52,  2.82s/it]loss: 2.6197, accuracy: 0.5938:  63%|██████▎   | 66/105 [03:35<01:53,  2.91s/it]loss: 2.4707, accuracy: 0.6562:  63%|██████▎   | 66/105 [03:35<01:53,  2.91s/it]loss: 2.4707, accuracy: 0.6562:  64%|██████▍   | 67/105 [03:38<01:49,  2.89s/it]loss: 2.4820, accuracy: 0.7188:  64%|██████▍   | 67/105 [03:38<01:49,  2.89s/it]loss: 2.4820, accuracy: 0.7188:  65%|██████▍   | 68/105 [03:40<01:47,  2.90s/it]loss: 2.2137, accuracy: 0.7812:  65%|██████▍   | 68/105 [03:40<01:47,  2.90s/it]loss: 2.2137, accuracy: 0.7812:  66%|██████▌   | 69/105 [03:43<01:44,  2.89s/it]loss: 2.1622, accuracy: 0.7812:  66%|██████▌   | 69/105 [03:43<01:44,  2.89s/it]loss: 2.1622, accuracy: 0.7812:  67%|██████▋   | 70/105 [03:46<01:41,  2.90s/it]loss: 2.0315, accuracy: 0.6875:  67%|██████▋   | 70/105 [03:46<01:41,  2.90s/it]loss: 2.0315, accuracy: 0.6875:  68%|██████▊   | 71/105 [03:49<01:39,  2.93s/it]loss: 2.4872, accuracy: 0.6875:  68%|██████▊   | 71/105 [03:49<01:39,  2.93s/it]loss: 2.4872, accuracy: 0.6875:  69%|██████▊   | 72/105 [03:52<01:36,  2.92s/it]loss: 3.0063, accuracy: 0.5000:  69%|██████▊   | 72/105 [03:52<01:36,  2.92s/it]loss: 3.0063, accuracy: 0.5000:  70%|██████▉   | 73/105 [03:55<01:32,  2.90s/it]loss: 2.2663, accuracy: 0.6562:  70%|██████▉   | 73/105 [03:55<01:32,  2.90s/it]loss: 2.2663, accuracy: 0.6562:  70%|███████   | 74/105 [03:58<01:28,  2.87s/it]loss: 2.4458, accuracy: 0.6250:  70%|███████   | 74/105 [03:58<01:28,  2.87s/it]loss: 2.4458, accuracy: 0.6250:  71%|███████▏  | 75/105 [04:01<01:25,  2.86s/it]loss: 2.4390, accuracy: 0.6562:  71%|███████▏  | 75/105 [04:01<01:25,  2.86s/it]loss: 2.4390, accuracy: 0.6562:  72%|███████▏  | 76/105 [04:04<01:23,  2.89s/it]loss: 2.5016, accuracy: 0.7188:  72%|███████▏  | 76/105 [04:04<01:23,  2.89s/it]loss: 2.5016, accuracy: 0.7188:  73%|███████▎  | 77/105 [04:06<01:20,  2.87s/it]loss: 2.7186, accuracy: 0.6562:  73%|███████▎  | 77/105 [04:06<01:20,  2.87s/it]loss: 2.7186, accuracy: 0.6562:  74%|███████▍  | 78/105 [04:09<01:17,  2.88s/it]loss: 2.2369, accuracy: 0.7500:  74%|███████▍  | 78/105 [04:09<01:17,  2.88s/it]loss: 2.2369, accuracy: 0.7500:  75%|███████▌  | 79/105 [04:12<01:13,  2.84s/it]loss: 3.0114, accuracy: 0.5000:  75%|███████▌  | 79/105 [04:12<01:13,  2.84s/it]loss: 3.0114, accuracy: 0.5000:  76%|███████▌  | 80/105 [04:15<01:12,  2.90s/it]loss: 1.8075, accuracy: 0.7500:  76%|███████▌  | 80/105 [04:15<01:12,  2.90s/it]loss: 1.8075, accuracy: 0.7500:  77%|███████▋  | 81/105 [04:18<01:09,  2.88s/it]loss: 2.2287, accuracy: 0.8125:  77%|███████▋  | 81/105 [04:18<01:09,  2.88s/it]loss: 2.2287, accuracy: 0.8125:  78%|███████▊  | 82/105 [04:21<01:07,  2.93s/it]loss: 2.6821, accuracy: 0.5625:  78%|███████▊  | 82/105 [04:21<01:07,  2.93s/it]loss: 2.6821, accuracy: 0.5625:  79%|███████▉  | 83/105 [04:24<01:03,  2.88s/it]loss: 2.6060, accuracy: 0.6250:  79%|███████▉  | 83/105 [04:24<01:03,  2.88s/it]loss: 2.6060, accuracy: 0.6250:  80%|████████  | 84/105 [04:27<01:00,  2.89s/it]loss: 2.7358, accuracy: 0.6562:  80%|████████  | 84/105 [04:27<01:00,  2.89s/it]loss: 2.7358, accuracy: 0.6562:  81%|████████  | 85/105 [04:30<00:57,  2.86s/it]loss: 1.9212, accuracy: 0.8438:  81%|████████  | 85/105 [04:30<00:57,  2.86s/it]loss: 1.9212, accuracy: 0.8438:  82%|████████▏ | 86/105 [04:32<00:54,  2.85s/it]loss: 2.8285, accuracy: 0.6250:  82%|████████▏ | 86/105 [04:32<00:54,  2.85s/it]loss: 2.8285, accuracy: 0.6250:  83%|████████▎ | 87/105 [04:35<00:50,  2.83s/it]loss: 2.0553, accuracy: 0.7812:  83%|████████▎ | 87/105 [04:35<00:50,  2.83s/it]loss: 2.0553, accuracy: 0.7812:  84%|████████▍ | 88/105 [04:38<00:48,  2.87s/it]loss: 2.7043, accuracy: 0.6250:  84%|████████▍ | 88/105 [04:38<00:48,  2.87s/it]loss: 2.7043, accuracy: 0.6250:  85%|████████▍ | 89/105 [04:41<00:45,  2.87s/it]loss: 2.7529, accuracy: 0.5938:  85%|████████▍ | 89/105 [04:41<00:45,  2.87s/it]loss: 2.7529, accuracy: 0.5938:  86%|████████▌ | 90/105 [04:44<00:43,  2.87s/it]loss: 2.7608, accuracy: 0.5000:  86%|████████▌ | 90/105 [04:44<00:43,  2.87s/it]loss: 2.7608, accuracy: 0.5000:  87%|████████▋ | 91/105 [04:47<00:40,  2.88s/it]loss: 2.2376, accuracy: 0.6562:  87%|████████▋ | 91/105 [04:47<00:40,  2.88s/it]loss: 2.2376, accuracy: 0.6562:  88%|████████▊ | 92/105 [04:50<00:37,  2.90s/it]loss: 2.3740, accuracy: 0.7188:  88%|████████▊ | 92/105 [04:50<00:37,  2.90s/it]loss: 2.3740, accuracy: 0.7188:  89%|████████▊ | 93/105 [04:53<00:34,  2.89s/it]loss: 2.2428, accuracy: 0.7188:  89%|████████▊ | 93/105 [04:53<00:34,  2.89s/it]loss: 2.2428, accuracy: 0.7188:  90%|████████▉ | 94/105 [04:55<00:30,  2.82s/it]loss: 2.1717, accuracy: 0.7188:  90%|████████▉ | 94/105 [04:55<00:30,  2.82s/it]loss: 2.1717, accuracy: 0.7188:  90%|█████████ | 95/105 [04:58<00:27,  2.76s/it]loss: 2.1555, accuracy: 0.7500:  90%|█████████ | 95/105 [04:58<00:27,  2.76s/it]loss: 2.1555, accuracy: 0.7500:  91%|█████████▏| 96/105 [05:01<00:25,  2.81s/it]loss: 2.1481, accuracy: 0.7188:  91%|█████████▏| 96/105 [05:01<00:25,  2.81s/it]loss: 2.1481, accuracy: 0.7188:  92%|█████████▏| 97/105 [05:03<00:22,  2.77s/it]loss: 2.4726, accuracy: 0.5938:  92%|█████████▏| 97/105 [05:03<00:22,  2.77s/it]loss: 2.4726, accuracy: 0.5938:  93%|█████████▎| 98/105 [05:06<00:19,  2.79s/it]loss: 3.0691, accuracy: 0.5625:  93%|█████████▎| 98/105 [05:06<00:19,  2.79s/it]loss: 3.0691, accuracy: 0.5625:  94%|█████████▍| 99/105 [05:09<00:16,  2.83s/it]loss: 2.3314, accuracy: 0.6562:  94%|█████████▍| 99/105 [05:09<00:16,  2.83s/it]loss: 2.3314, accuracy: 0.6562:  95%|█████████▌| 100/105 [05:12<00:14,  2.88s/it]loss: 2.2942, accuracy: 0.6875:  95%|█████████▌| 100/105 [05:12<00:14,  2.88s/it]loss: 2.2942, accuracy: 0.6875:  96%|█████████▌| 101/105 [05:15<00:11,  2.89s/it]loss: 2.5954, accuracy: 0.5000:  96%|█████████▌| 101/105 [05:15<00:11,  2.89s/it]loss: 2.5954, accuracy: 0.5000:  97%|█████████▋| 102/105 [05:18<00:08,  2.88s/it]loss: 2.9650, accuracy: 0.4688:  97%|█████████▋| 102/105 [05:18<00:08,  2.88s/it]loss: 2.9650, accuracy: 0.4688:  98%|█████████▊| 103/105 [05:21<00:05,  2.89s/it]loss: 2.5864, accuracy: 0.6250:  98%|█████████▊| 103/105 [05:21<00:05,  2.89s/it]loss: 2.5864, accuracy: 0.6250:  99%|█████████▉| 104/105 [05:24<00:02,  2.84s/it]loss: 2.8355, accuracy: 0.4688:  99%|█████████▉| 104/105 [05:24<00:02,  2.84s/it]loss: 2.8355, accuracy: 0.4688: 100%|██████████| 105/105 [05:26<00:00,  2.62s/it]loss: 3.0168, accuracy: 0.4000: 100%|██████████| 105/105 [05:26<00:00,  2.62s/it]loss: 3.0168, accuracy: 0.4000: 100%|██████████| 105/105 [05:26<00:00,  3.11s/it]
