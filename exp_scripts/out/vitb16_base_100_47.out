Namespace(batch_size=64, continue_from_epoch=-2, seed=0, num_epochs=5, experiment_name='vitb16_base_100', use_gpu=True, weight_decay_coefficient=0.0005, learning_rate=0.001, model='vitb16', pretrain='base', dataloader='birds', height=100, width=100)
Number of training samples:  4495
Number of validation samples:  1499
Number of testing samples:  5794
Number of classes: 200
Use Multi GPU 0
here
System learnable parameters
DataParallel(
  (module): VisionTransformer(
    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (encoder): Encoder(
      (dropout): Dropout(p=0.0, inplace=False)
      (layers): Sequential(
        (encoder_layer_0): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_1): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_2): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_3): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_4): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_5): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_6): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_7): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_8): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_9): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_10): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_11): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (heads): Sequential(
      (head): Linear(in_features=768, out_features=200, bias=True)
    )
  )
)
  0%|          | 0/71 [00:00<?, ?it/s]  1%|▏         | 1/71 [00:11<13:35, 11.65s/it]loss: 1.9789, accuracy: 0.8438:   1%|▏         | 1/71 [00:11<13:35, 11.65s/it]loss: 1.9789, accuracy: 0.8438:   3%|▎         | 2/71 [00:13<06:48,  5.92s/it]loss: 1.8115, accuracy: 0.8906:   3%|▎         | 2/71 [00:13<06:48,  5.92s/it]loss: 1.8115, accuracy: 0.8906:   4%|▍         | 3/71 [00:15<04:37,  4.07s/it]loss: 1.9766, accuracy: 0.8438:   4%|▍         | 3/71 [00:15<04:37,  4.07s/it]loss: 1.9766, accuracy: 0.8438:   6%|▌         | 4/71 [00:17<03:34,  3.20s/it]loss: 1.9771, accuracy: 0.8125:   6%|▌         | 4/71 [00:17<03:34,  3.20s/it]loss: 1.9771, accuracy: 0.8125:   7%|▋         | 5/71 [00:19<02:59,  2.73s/it]loss: 1.8190, accuracy: 0.8906:   7%|▋         | 5/71 [00:19<02:59,  2.73s/it]loss: 1.8190, accuracy: 0.8906:   8%|▊         | 6/71 [00:21<02:38,  2.44s/it]loss: 2.0207, accuracy: 0.7656:   8%|▊         | 6/71 [00:21<02:38,  2.44s/it]loss: 2.0207, accuracy: 0.7656:  10%|▉         | 7/71 [00:22<02:24,  2.25s/it]loss: 1.8504, accuracy: 0.8750:  10%|▉         | 7/71 [00:22<02:24,  2.25s/it]loss: 1.8504, accuracy: 0.8750:  11%|█▏        | 8/71 [00:24<02:14,  2.13s/it]loss: 1.7335, accuracy: 0.9219:  11%|█▏        | 8/71 [00:24<02:14,  2.13s/it]loss: 1.7335, accuracy: 0.9219:  13%|█▎        | 9/71 [00:26<02:06,  2.05s/it]loss: 1.5748, accuracy: 0.9688:  13%|█▎        | 9/71 [00:26<02:06,  2.05s/it]loss: 1.5748, accuracy: 0.9688:  14%|█▍        | 10/71 [00:28<02:01,  2.00s/it]loss: 1.6637, accuracy: 0.9531:  14%|█▍        | 10/71 [00:28<02:01,  2.00s/it]loss: 1.6637, accuracy: 0.9531:  15%|█▌        | 11/71 [00:30<01:57,  1.96s/it]loss: 1.7486, accuracy: 0.9375:  15%|█▌        | 11/71 [00:30<01:57,  1.96s/it]loss: 1.7486, accuracy: 0.9375:  17%|█▋        | 12/71 [00:32<01:53,  1.93s/it]loss: 1.7962, accuracy: 0.8906:  17%|█▋        | 12/71 [00:32<01:53,  1.93s/it]loss: 1.7962, accuracy: 0.8906:  18%|█▊        | 13/71 [00:34<01:51,  1.92s/it]loss: 1.9394, accuracy: 0.8438:  18%|█▊        | 13/71 [00:34<01:51,  1.92s/it]loss: 1.9394, accuracy: 0.8438:  20%|█▉        | 14/71 [00:36<01:50,  1.94s/it]loss: 1.7771, accuracy: 0.9062:  20%|█▉        | 14/71 [00:36<01:50,  1.94s/it]loss: 1.7771, accuracy: 0.9062:  21%|██        | 15/71 [00:38<01:47,  1.92s/it]loss: 1.9455, accuracy: 0.9062:  21%|██        | 15/71 [00:38<01:47,  1.92s/it]loss: 1.9455, accuracy: 0.9062:  23%|██▎       | 16/71 [00:39<01:44,  1.91s/it]loss: 1.7773, accuracy: 0.8906:  23%|██▎       | 16/71 [00:39<01:44,  1.91s/it]loss: 1.7773, accuracy: 0.8906:  24%|██▍       | 17/71 [00:41<01:42,  1.91s/it]loss: 1.8181, accuracy: 0.8750:  24%|██▍       | 17/71 [00:41<01:42,  1.91s/it]loss: 1.8181, accuracy: 0.8750:  25%|██▌       | 18/71 [00:43<01:40,  1.89s/it]loss: 1.7806, accuracy: 0.9062:  25%|██▌       | 18/71 [00:43<01:40,  1.89s/it]loss: 1.7806, accuracy: 0.9062:  27%|██▋       | 19/71 [00:45<01:38,  1.89s/it]loss: 1.5803, accuracy: 0.9688:  27%|██▋       | 19/71 [00:45<01:38,  1.89s/it]loss: 1.5803, accuracy: 0.9688:  28%|██▊       | 20/71 [00:47<01:36,  1.89s/it]loss: 1.9251, accuracy: 0.8594:  28%|██▊       | 20/71 [00:47<01:36,  1.89s/it]loss: 1.9251, accuracy: 0.8594:  30%|██▉       | 21/71 [00:49<01:34,  1.89s/it]loss: 1.7522, accuracy: 0.8906:  30%|██▉       | 21/71 [00:49<01:34,  1.89s/it]loss: 1.7522, accuracy: 0.8906:  31%|███       | 22/71 [00:51<01:32,  1.88s/it]loss: 1.7754, accuracy: 0.9219:  31%|███       | 22/71 [00:51<01:32,  1.88s/it]loss: 1.7754, accuracy: 0.9219:  32%|███▏      | 23/71 [00:53<01:30,  1.88s/it]loss: 1.7458, accuracy: 0.9375:  32%|███▏      | 23/71 [00:53<01:30,  1.88s/it]loss: 1.7458, accuracy: 0.9375:  34%|███▍      | 24/71 [00:54<01:28,  1.88s/it]loss: 1.9427, accuracy: 0.8594:  34%|███▍      | 24/71 [00:54<01:28,  1.88s/it]loss: 1.9427, accuracy: 0.8594:  35%|███▌      | 25/71 [00:56<01:26,  1.87s/it]loss: 2.0911, accuracy: 0.7969:  35%|███▌      | 25/71 [00:56<01:26,  1.87s/it]loss: 2.0911, accuracy: 0.7969:  37%|███▋      | 26/71 [00:58<01:25,  1.91s/it]loss: 1.8525, accuracy: 0.8281:  37%|███▋      | 26/71 [00:58<01:25,  1.91s/it]loss: 1.8525, accuracy: 0.8281:  38%|███▊      | 27/71 [01:00<01:23,  1.90s/it]loss: 1.7594, accuracy: 0.9062:  38%|███▊      | 27/71 [01:00<01:23,  1.90s/it]loss: 1.7594, accuracy: 0.9062:  39%|███▉      | 28/71 [01:02<01:21,  1.89s/it]loss: 1.7376, accuracy: 0.9531:  39%|███▉      | 28/71 [01:02<01:21,  1.89s/it]loss: 1.7376, accuracy: 0.9531:  41%|████      | 29/71 [01:04<01:19,  1.89s/it]loss: 1.6442, accuracy: 0.9062:  41%|████      | 29/71 [01:04<01:19,  1.89s/it]loss: 1.6442, accuracy: 0.9062:  42%|████▏     | 30/71 [01:06<01:17,  1.89s/it]loss: 1.6964, accuracy: 0.9219:  42%|████▏     | 30/71 [01:06<01:17,  1.89s/it]loss: 1.6964, accuracy: 0.9219:  44%|████▎     | 31/71 [01:08<01:15,  1.89s/it]loss: 1.8121, accuracy: 0.8750:  44%|████▎     | 31/71 [01:08<01:15,  1.89s/it]loss: 1.8121, accuracy: 0.8750:  45%|████▌     | 32/71 [01:10<01:13,  1.89s/it]loss: 1.8629, accuracy: 0.9062:  45%|████▌     | 32/71 [01:10<01:13,  1.89s/it]loss: 1.8629, accuracy: 0.9062:  46%|████▋     | 33/71 [01:11<01:11,  1.88s/it]loss: 1.8938, accuracy: 0.8594:  46%|████▋     | 33/71 [01:11<01:11,  1.88s/it]loss: 1.8938, accuracy: 0.8594:  48%|████▊     | 34/71 [01:13<01:09,  1.89s/it]loss: 1.9758, accuracy: 0.8750:  48%|████▊     | 34/71 [01:13<01:09,  1.89s/it]loss: 1.9758, accuracy: 0.8750:  49%|████▉     | 35/71 [01:15<01:07,  1.89s/it]loss: 1.9590, accuracy: 0.7969:  49%|████▉     | 35/71 [01:15<01:07,  1.89s/it]loss: 1.9590, accuracy: 0.7969:  51%|█████     | 36/71 [01:17<01:05,  1.88s/it]loss: 1.9666, accuracy: 0.8906:  51%|█████     | 36/71 [01:17<01:05,  1.88s/it]loss: 1.9666, accuracy: 0.8906:  52%|█████▏    | 37/71 [01:19<01:04,  1.89s/it]loss: 1.8058, accuracy: 0.8750:  52%|█████▏    | 37/71 [01:19<01:04,  1.89s/it]loss: 1.8058, accuracy: 0.8750:  54%|█████▎    | 38/71 [01:21<01:01,  1.87s/it]loss: 1.9212, accuracy: 0.8594:  54%|█████▎    | 38/71 [01:21<01:01,  1.87s/it]loss: 1.9212, accuracy: 0.8594:  55%|█████▍    | 39/71 [01:23<01:01,  1.91s/it]loss: 1.9473, accuracy: 0.8281:  55%|█████▍    | 39/71 [01:23<01:01,  1.91s/it]loss: 1.9473, accuracy: 0.8281:  56%|█████▋    | 40/71 [01:25<00:58,  1.90s/it]loss: 1.6920, accuracy: 0.8906:  56%|█████▋    | 40/71 [01:25<00:58,  1.90s/it]loss: 1.6920, accuracy: 0.8906:  58%|█████▊    | 41/71 [01:27<00:56,  1.89s/it]loss: 1.7805, accuracy: 0.9062:  58%|█████▊    | 41/71 [01:27<00:56,  1.89s/it]loss: 1.7805, accuracy: 0.9062:  59%|█████▉    | 42/71 [01:28<00:54,  1.88s/it]loss: 2.0567, accuracy: 0.7969:  59%|█████▉    | 42/71 [01:28<00:54,  1.88s/it]loss: 2.0567, accuracy: 0.7969:  61%|██████    | 43/71 [01:30<00:52,  1.88s/it]loss: 2.0151, accuracy: 0.7812:  61%|██████    | 43/71 [01:30<00:52,  1.88s/it]loss: 2.0151, accuracy: 0.7812:  62%|██████▏   | 44/71 [01:32<00:50,  1.88s/it]loss: 2.2735, accuracy: 0.7031:  62%|██████▏   | 44/71 [01:32<00:50,  1.88s/it]loss: 2.2735, accuracy: 0.7031:  63%|██████▎   | 45/71 [01:34<00:49,  1.89s/it]loss: 1.8835, accuracy: 0.8750:  63%|██████▎   | 45/71 [01:34<00:49,  1.89s/it]loss: 1.8835, accuracy: 0.8750:  65%|██████▍   | 46/71 [01:36<00:47,  1.89s/it]loss: 1.8701, accuracy: 0.9062:  65%|██████▍   | 46/71 [01:36<00:47,  1.89s/it]loss: 1.8701, accuracy: 0.9062:  66%|██████▌   | 47/71 [01:38<00:45,  1.88s/it]loss: 1.7525, accuracy: 0.9219:  66%|██████▌   | 47/71 [01:38<00:45,  1.88s/it]loss: 1.7525, accuracy: 0.9219:  68%|██████▊   | 48/71 [01:40<00:43,  1.88s/it]loss: 1.8165, accuracy: 0.8750:  68%|██████▊   | 48/71 [01:40<00:43,  1.88s/it]loss: 1.8165, accuracy: 0.8750:  69%|██████▉   | 49/71 [01:42<00:41,  1.88s/it]loss: 1.8425, accuracy: 0.9062:  69%|██████▉   | 49/71 [01:42<00:41,  1.88s/it]loss: 1.8425, accuracy: 0.9062:  70%|███████   | 50/71 [01:44<00:39,  1.88s/it]loss: 1.6924, accuracy: 0.9531:  70%|███████   | 50/71 [01:44<00:39,  1.88s/it]loss: 1.6924, accuracy: 0.9531:  72%|███████▏  | 51/71 [01:46<00:38,  1.91s/it]loss: 1.6746, accuracy: 0.9375:  72%|███████▏  | 51/71 [01:46<00:38,  1.91s/it]loss: 1.6746, accuracy: 0.9375:  73%|███████▎  | 52/71 [01:47<00:36,  1.90s/it]loss: 1.9991, accuracy: 0.8438:  73%|███████▎  | 52/71 [01:47<00:36,  1.90s/it]loss: 1.9991, accuracy: 0.8438:  75%|███████▍  | 53/71 [01:49<00:34,  1.89s/it]loss: 2.0156, accuracy: 0.8906:  75%|███████▍  | 53/71 [01:49<00:34,  1.89s/it]loss: 2.0156, accuracy: 0.8906:  76%|███████▌  | 54/71 [01:51<00:32,  1.89s/it]loss: 2.1312, accuracy: 0.7500:  76%|███████▌  | 54/71 [01:51<00:32,  1.89s/it]loss: 2.1312, accuracy: 0.7500:  77%|███████▋  | 55/71 [01:53<00:30,  1.88s/it]loss: 1.9541, accuracy: 0.8438:  77%|███████▋  | 55/71 [01:53<00:30,  1.88s/it]loss: 1.9541, accuracy: 0.8438:  79%|███████▉  | 56/71 [01:55<00:28,  1.88s/it]loss: 2.1650, accuracy: 0.7031:  79%|███████▉  | 56/71 [01:55<00:28,  1.88s/it]loss: 2.1650, accuracy: 0.7031:  80%|████████  | 57/71 [01:57<00:26,  1.88s/it]loss: 1.9296, accuracy: 0.8281:  80%|████████  | 57/71 [01:57<00:26,  1.88s/it]loss: 1.9296, accuracy: 0.8281:  82%|████████▏ | 58/71 [01:59<00:24,  1.88s/it]loss: 1.7717, accuracy: 0.9062:  82%|████████▏ | 58/71 [01:59<00:24,  1.88s/it]loss: 1.7717, accuracy: 0.9062:  83%|████████▎ | 59/71 [02:01<00:22,  1.88s/it]loss: 1.7171, accuracy: 0.8906:  83%|████████▎ | 59/71 [02:01<00:22,  1.88s/it]loss: 1.7171, accuracy: 0.8906:  85%|████████▍ | 60/71 [02:02<00:20,  1.88s/it]loss: 1.6857, accuracy: 0.9531:  85%|████████▍ | 60/71 [02:02<00:20,  1.88s/it]loss: 1.6857, accuracy: 0.9531:  86%|████████▌ | 61/71 [02:04<00:18,  1.88s/it]loss: 1.8801, accuracy: 0.9062:  86%|████████▌ | 61/71 [02:04<00:18,  1.88s/it]loss: 1.8801, accuracy: 0.9062:  87%|████████▋ | 62/71 [02:06<00:16,  1.87s/it]loss: 1.9978, accuracy: 0.7969:  87%|████████▋ | 62/71 [02:06<00:16,  1.87s/it]loss: 1.9978, accuracy: 0.7969:  89%|████████▊ | 63/71 [02:08<00:15,  1.88s/it]loss: 2.0637, accuracy: 0.8438:  89%|████████▊ | 63/71 [02:08<00:15,  1.88s/it]loss: 2.0637, accuracy: 0.8438:  90%|█████████ | 64/71 [02:10<00:13,  1.91s/it]loss: 2.2019, accuracy: 0.6562:  90%|█████████ | 64/71 [02:10<00:13,  1.91s/it]loss: 2.2019, accuracy: 0.6562:  92%|█████████▏| 65/71 [02:12<00:11,  1.89s/it]loss: 2.0355, accuracy: 0.7500:  92%|█████████▏| 65/71 [02:12<00:11,  1.89s/it]loss: 2.0355, accuracy: 0.7500:  93%|█████████▎| 66/71 [02:14<00:09,  1.88s/it]loss: 1.9882, accuracy: 0.8125:  93%|█████████▎| 66/71 [02:14<00:09,  1.88s/it]loss: 1.9882, accuracy: 0.8125:  94%|█████████▍| 67/71 [02:16<00:07,  1.88s/it]loss: 1.9636, accuracy: 0.7656:  94%|█████████▍| 67/71 [02:16<00:07,  1.88s/it]loss: 1.9636, accuracy: 0.7656:  96%|█████████▌| 68/71 [02:18<00:05,  1.89s/it]loss: 1.9477, accuracy: 0.7812:  96%|█████████▌| 68/71 [02:18<00:05,  1.89s/it]loss: 1.9477, accuracy: 0.7812:  97%|█████████▋| 69/71 [02:19<00:03,  1.87s/it]loss: 1.7082, accuracy: 0.9375:  97%|█████████▋| 69/71 [02:19<00:03,  1.87s/it]loss: 1.7082, accuracy: 0.9375:  99%|█████████▊| 70/71 [02:21<00:01,  1.87s/it]loss: 1.9101, accuracy: 0.7969:  99%|█████████▊| 70/71 [02:21<00:01,  1.87s/it]loss: 1.9101, accuracy: 0.7969: 100%|██████████| 71/71 [02:22<00:00,  1.63s/it]loss: 1.2321, accuracy: 1.0000: 100%|██████████| 71/71 [02:22<00:00,  1.63s/it]loss: 1.2321, accuracy: 1.0000: 100%|██████████| 71/71 [02:22<00:00,  2.01s/it]
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:29,  1.27s/it]loss: 5.3659, accuracy: 0.1406:   4%|▍         | 1/24 [00:01<00:29,  1.27s/it]loss: 5.3659, accuracy: 0.1406:   8%|▊         | 2/24 [00:02<00:28,  1.28s/it]loss: 5.3590, accuracy: 0.0469:   8%|▊         | 2/24 [00:02<00:28,  1.28s/it]loss: 5.3590, accuracy: 0.0469:  12%|█▎        | 3/24 [00:03<00:26,  1.28s/it]loss: 5.6839, accuracy: 0.0469:  12%|█▎        | 3/24 [00:03<00:26,  1.28s/it]loss: 5.6839, accuracy: 0.0469:  17%|█▋        | 4/24 [00:05<00:25,  1.29s/it]loss: 5.3532, accuracy: 0.0938:  17%|█▋        | 4/24 [00:05<00:25,  1.29s/it]loss: 5.3532, accuracy: 0.0938:  21%|██        | 5/24 [00:06<00:24,  1.28s/it]loss: 5.3485, accuracy: 0.0938:  21%|██        | 5/24 [00:06<00:24,  1.28s/it]loss: 5.3485, accuracy: 0.0938:  25%|██▌       | 6/24 [00:07<00:23,  1.28s/it]loss: 4.8793, accuracy: 0.1719:  25%|██▌       | 6/24 [00:07<00:23,  1.28s/it]loss: 4.8793, accuracy: 0.1719:  29%|██▉       | 7/24 [00:08<00:21,  1.28s/it]loss: 5.6103, accuracy: 0.0625:  29%|██▉       | 7/24 [00:08<00:21,  1.28s/it]loss: 5.6103, accuracy: 0.0625:  33%|███▎      | 8/24 [00:10<00:20,  1.30s/it]loss: 5.3041, accuracy: 0.0625:  33%|███▎      | 8/24 [00:10<00:20,  1.30s/it]loss: 5.3041, accuracy: 0.0625:  38%|███▊      | 9/24 [00:11<00:19,  1.30s/it]loss: 5.5020, accuracy: 0.0469:  38%|███▊      | 9/24 [00:11<00:19,  1.30s/it]loss: 5.5020, accuracy: 0.0469:  42%|████▏     | 10/24 [00:12<00:18,  1.30s/it]loss: 5.6573, accuracy: 0.0469:  42%|████▏     | 10/24 [00:12<00:18,  1.30s/it]loss: 5.6573, accuracy: 0.0469:  46%|████▌     | 11/24 [00:14<00:16,  1.30s/it]loss: 5.1055, accuracy: 0.0938:  46%|████▌     | 11/24 [00:14<00:16,  1.30s/it]loss: 5.1055, accuracy: 0.0938:  50%|█████     | 12/24 [00:15<00:15,  1.30s/it]loss: 5.2777, accuracy: 0.0625:  50%|█████     | 12/24 [00:15<00:15,  1.30s/it]loss: 5.2777, accuracy: 0.0625:  54%|█████▍    | 13/24 [00:16<00:14,  1.30s/it]loss: 5.4419, accuracy: 0.0469:  54%|█████▍    | 13/24 [00:16<00:14,  1.30s/it]loss: 5.4419, accuracy: 0.0469:  58%|█████▊    | 14/24 [00:18<00:12,  1.30s/it]loss: 5.3064, accuracy: 0.1094:  58%|█████▊    | 14/24 [00:18<00:12,  1.30s/it]loss: 5.3064, accuracy: 0.1094:  62%|██████▎   | 15/24 [00:19<00:11,  1.29s/it]loss: 5.4433, accuracy: 0.0469:  62%|██████▎   | 15/24 [00:19<00:11,  1.29s/it]loss: 5.4433, accuracy: 0.0469:  67%|██████▋   | 16/24 [00:20<00:10,  1.28s/it]loss: 5.4173, accuracy: 0.0938:  67%|██████▋   | 16/24 [00:20<00:10,  1.28s/it]loss: 5.4173, accuracy: 0.0938:  71%|███████   | 17/24 [00:21<00:08,  1.28s/it]loss: 5.1547, accuracy: 0.0781:  71%|███████   | 17/24 [00:21<00:08,  1.28s/it]loss: 5.1547, accuracy: 0.0781:  75%|███████▌  | 18/24 [00:23<00:07,  1.28s/it]loss: 5.1985, accuracy: 0.0938:  75%|███████▌  | 18/24 [00:23<00:07,  1.28s/it]loss: 5.1985, accuracy: 0.0938:  79%|███████▉  | 19/24 [00:24<00:06,  1.28s/it]loss: 5.3729, accuracy: 0.1094:  79%|███████▉  | 19/24 [00:24<00:06,  1.28s/it]loss: 5.3729, accuracy: 0.1094:  83%|████████▎ | 20/24 [00:25<00:05,  1.28s/it]loss: 5.2842, accuracy: 0.0781:  83%|████████▎ | 20/24 [00:25<00:05,  1.28s/it]loss: 5.2842, accuracy: 0.0781:  88%|████████▊ | 21/24 [00:27<00:03,  1.32s/it]loss: 5.4251, accuracy: 0.0781:  88%|████████▊ | 21/24 [00:27<00:03,  1.32s/it]loss: 5.4251, accuracy: 0.0781:  92%|█████████▏| 22/24 [00:28<00:02,  1.30s/it]loss: 5.2862, accuracy: 0.0469:  92%|█████████▏| 22/24 [00:28<00:02,  1.30s/it]loss: 5.2862, accuracy: 0.0469:  96%|█████████▌| 23/24 [00:29<00:01,  1.29s/it]loss: 5.4101, accuracy: 0.0625:  96%|█████████▌| 23/24 [00:29<00:01,  1.29s/it]loss: 5.4101, accuracy: 0.0625: 100%|██████████| 24/24 [00:33<00:00,  1.98s/it]loss: 5.8271, accuracy: 0.0000: 100%|██████████| 24/24 [00:33<00:00,  1.98s/it]loss: 5.8271, accuracy: 0.0000: 100%|██████████| 24/24 [00:33<00:00,  1.39s/it]
Epoch 84: epoch_84.0000_train_acc_0.8669_train_loss_1.8632_val_acc_0.0755_val_loss_5.3756 epoch time 176.0598 seconds
Traceback (most recent call last):
  File "/home/s2016765/miniconda3/envs/mlp/lib/python3.12/site-packages/torch/serialization.py", line 629, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/home/s2016765/miniconda3/envs/mlp/lib/python3.12/site-packages/torch/serialization.py", line 863, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/72: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/s2016765/final_project/train_models.py", line 159, in <module>
    experiment_metrics, test_metrics = conv_experiment.run_experiment()  # run experiment and return experiment metrics
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s2016765/final_project/pytorch_mlp_framework/experiment_builder.py", line 294, in run_experiment
    self.save_model(model_save_dir=self.experiment_saved_models,
  File "/home/s2016765/final_project/pytorch_mlp_framework/experiment_builder.py", line 232, in save_model
    torch.save(self.state, f=os.path.join(model_save_dir, "{}_{}".format(model_save_name, str(
  File "/home/s2016765/miniconda3/envs/mlp/lib/python3.12/site-packages/torch/serialization.py", line 628, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/s2016765/miniconda3/envs/mlp/lib/python3.12/site-packages/torch/serialization.py", line 476, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 268366528 vs 268366416
